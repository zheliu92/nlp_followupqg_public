{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n",
    "sys.path.append(os.path.join(base_dir, \"Data_Augmentation\"))\n",
    "\n",
    "from llm import llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_json('../informativeness_output/full_df_with_answerability.json')\n",
    "org_df = pd.read_json('../informativeness_output/org_df_with_answerability.json')\n",
    "gpt_df = pd.read_json('../informativeness_output/gpt_df_with_answerability.json')\n",
    "\n",
    "ERROR_MSG = \"LLM failed to generate a response\"\n",
    "\n",
    "idx_with_incorrect_eval = []\n",
    "example_generated_follow_up = []\n",
    "\n",
    "def analyze_informativeness(df):\n",
    "    counter = Counter()\n",
    "    \n",
    "    for idx, data in df.iterrows():\n",
    "        follow_up_answerability = data[\"generated_follow_up_answerability\"].replace(\"“\", '\"').replace(\"”\", '\"').replace(\"'\", '\"')\n",
    "\n",
    "        if follow_up_answerability == ERROR_MSG: continue\n",
    "\n",
    "        nested_list = json.loads(follow_up_answerability)\n",
    "\n",
    "        if len(nested_list) != len(data[\"generated_follow_up\"]):\n",
    "            idx_with_incorrect_eval.append(idx)\n",
    "            example_generated_follow_up.append(data)\n",
    "\n",
    "        if ('Complete Answer','Original Answer') in nested_list:\n",
    "            print(idx)\n",
    "\n",
    "        counter.update(tuple(sublist) for sublist in nested_list)\n",
    "\n",
    "    return counter\n",
    "\n",
    "full_result = analyze_informativeness(full_df)\n",
    "org_result = analyze_informativeness(org_df)\n",
    "gpt_result = analyze_informativeness(gpt_df)\n",
    "print(idx_with_incorrect_eval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Complete Answer',): 712,\n",
       "         (): 676,\n",
       "         ('Original Answer', 'Complete Answer'): 397,\n",
       "         ('Original Answer',): 255})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Original Answer', 'Complete Answer'): 610,\n",
       "         (): 592,\n",
       "         ('Original Answer',): 582,\n",
       "         ('Complete Answer',): 542,\n",
       "         ('Complete Answer', 'Original Answer'): 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Original Answer', 'Complete Answer'): 611,\n",
       "         (): 592,\n",
       "         ('Original Answer',): 582,\n",
       "         ('Complete Answer',): 542})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the count of ('Complete Answer', 'Original Answer') to ('Original Answer', 'Complete Answer')\n",
    "org_result[('Original Answer', 'Complete Answer')] += org_result[('Complete Answer', 'Original Answer')]\n",
    "\n",
    "# Remove the ('Complete Answer', 'Original Answer') entry\n",
    "del org_result[('Complete Answer', 'Original Answer')]\n",
    "\n",
    "org_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Complete Answer',): 674,\n",
       "         (): 624,\n",
       "         ('Original Answer', 'Complete Answer'): 400,\n",
       "         ('Original Answer',): 179})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_table = pd.DataFrame([\n",
    "    [\"Generated FollowupQ\", \"Answered by CA\", \"Unanswered by CA\"],\n",
    "    [\"Answered by OA\", \"No new information\", \"Inappropriate CA\"],\n",
    "    [\"Unanswered by OA\", \"New Information\", \"Unrelated followupQ\"]\n",
    "])\n",
    "\n",
    "def display_result(df, table):\n",
    "    count = 0\n",
    "\n",
    "    for value in df.values():\n",
    "        count += value\n",
    "\n",
    "    for key, value in df.items():\n",
    "        percentage = round((value / count), 2)\n",
    "        match key:\n",
    "            case ():\n",
    "                table.iloc[2,2] = percentage\n",
    "            case ('Complete Answer',):\n",
    "                table.iloc[2,1] = percentage\n",
    "            case ('Original Answer',):\n",
    "                table.iloc[1,2] = percentage\n",
    "            case ('Original Answer', 'Complete Answer'):\n",
    "                table.iloc[1,1] = percentage\n",
    "    \n",
    "    return table\n",
    "\n",
    "full_result_table = display_result(full_result, template_table.copy())\n",
    "org_result_table = display_result(org_result, template_table.copy())\n",
    "gpt_result_table = display_result(gpt_result, template_table.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generated FollowupQ</td>\n",
       "      <td>Answered by CA</td>\n",
       "      <td>Unanswered by CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answered by OA</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unanswered by OA</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0               1                 2\n",
       "0  Generated FollowupQ  Answered by CA  Unanswered by CA\n",
       "1       Answered by OA            0.19              0.12\n",
       "2     Unanswered by OA            0.35              0.33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generated FollowupQ</td>\n",
       "      <td>Answered by CA</td>\n",
       "      <td>Unanswered by CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answered by OA</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unanswered by OA</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0               1                 2\n",
       "0  Generated FollowupQ  Answered by CA  Unanswered by CA\n",
       "1       Answered by OA            0.26              0.25\n",
       "2     Unanswered by OA            0.23              0.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generated FollowupQ</td>\n",
       "      <td>Answered by CA</td>\n",
       "      <td>Unanswered by CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answered by OA</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unanswered by OA</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0               1                 2\n",
       "0  Generated FollowupQ  Answered by CA  Unanswered by CA\n",
       "1       Answered by OA            0.21               0.1\n",
       "2     Unanswered by OA            0.36              0.33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_result_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
