{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>prefix</th>\n",
       "      <th>newInformation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3001_1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       group  prefix  newInformation\n",
       "0     full_1  3000_1             NaN\n",
       "1     full_1  3000_2             2.0\n",
       "2     full_1  3000_3             2.0\n",
       "3     full_1  3000_4             2.0\n",
       "4     full_1  3001_1             2.0\n",
       "...      ...     ...             ...\n",
       "1123  org_10  3099_3             0.0\n",
       "1124  org_10  3099_4             0.0\n",
       "1125  org_10  3099_5             0.0\n",
       "1126  org_10  3099_6             1.0\n",
       "1127  org_10  3099_7             2.0\n",
       "\n",
       "[1128 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human Evaluation Data\n",
    "df_human = pd.read_csv(\"../mode_2/human_eval_all_questions_annotated.csv\") # only valid FUQs\n",
    "df_new_info_human = df_human[[\"group\", \"prefix\", \"newInformation\"]].copy()\n",
    "df_new_info_human[\"group\"] = df_new_info_human[\"group\"].astype(pd.StringDtype())\n",
    "df_new_info_human[\"prefix\"] = df_new_info_human[\"prefix\"].astype(pd.StringDtype())\n",
    "df_new_info_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Evaluation Data\n",
    "dfs_auto = {\n",
    "    \"full\": pd.read_json('informativeness_output/full_df_with_answerability.json'),\n",
    "    \"org\": pd.read_json('informativeness_output/org_df_with_answerability.json'),\n",
    "    \"gpt\": pd.read_json('informativeness_output/gpt_df_with_answerability.json')\n",
    "}\n",
    "\n",
    "ERROR_MSG = \"LLM failed to generate a response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "follow_up_scores = {\n",
    "    \"full\": {\n",
    "        \"informative\":[],\n",
    "        \"not_informative\":[]\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"informative\":[],\n",
    "        \"not_informative\":[]\n",
    "    },\n",
    "    \"org\": {\n",
    "        \"informative\":[],\n",
    "        \"not_informative\":[]\n",
    "    },\n",
    "}\n",
    "\n",
    "informative_follow_up_scores = []\n",
    "not_informative_follow_up_scores = []\n",
    "ca = \"complete answer\"\n",
    "oa = \"original answer\"\n",
    "matches = 0\n",
    "\n",
    "for df_name, df in dfs_auto.items():\n",
    "    for _, data in df.iterrows():\n",
    "        id = data[\"id\"]\n",
    "        follow_up_idx = 1\n",
    "        follow_up_answerability = data[\"generated_follow_up_answerability\"].replace(\"“\", '\"').replace(\"”\", '\"').replace(\"'\", '\"')\n",
    "\n",
    "        if follow_up_answerability == ERROR_MSG: continue\n",
    "\n",
    "        follow_up_answerability = json.loads(follow_up_answerability)\n",
    "        \n",
    "        for follow_up in follow_up_answerability:\n",
    "            matching_row = df_new_info_human[\n",
    "                (df_new_info_human[\"prefix\"] == f\"{id}_{follow_up_idx}\") &\n",
    "                (df_new_info_human[\"group\"].str.startswith(df_name))\n",
    "            ]\n",
    "\n",
    "            if not matching_row.empty:\n",
    "                matches+=1\n",
    "                if follow_up and follow_up[0].lower() == ca:\n",
    "                    follow_up_scores[df_name][\"informative\"].extend(matching_row[\"newInformation\"].tolist())\n",
    "\n",
    "                    # informative_follow_up_scores.extend(matching_row[\"newInformation\"].tolist())\n",
    "                else:\n",
    "                    follow_up_scores[df_name][\"not_informative\"].extend(matching_row[\"newInformation\"].tolist())\n",
    "                    # not_informative_follow_up_scores.extend(matching_row[\"newInformation\"].tolist())\n",
    "                    \n",
    "            follow_up_idx+=1\n",
    "            \n",
    "print(matches)\n",
    "# len(informative_follow_up_scores + not_informative_follow_up_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/Var of values obtained from Human Labelling for \n",
      "Questions classified as Informative/Not Informative by GPT-4o\n",
      "FULL\n",
      "                    Category      Mean  Variance\n",
      "0      Informative Follow-Up  1.421053  0.699908\n",
      "1  Not Informative Follow-Up  1.237500  0.881094\n",
      "GPT\n",
      "                    Category      Mean  Variance\n",
      "0      Informative Follow-Up  1.578947  0.998153\n",
      "1  Not Informative Follow-Up  1.398010  1.194822\n",
      "ORG\n",
      "                    Category      Mean  Variance\n",
      "0      Informative Follow-Up  0.861111  1.064043\n",
      "1  Not Informative Follow-Up  0.753894  0.926971\n"
     ]
    }
   ],
   "source": [
    "def informative_score_per_model(df_name):\n",
    "    data = {\n",
    "        \"Category\": [\"Informative Follow-Up\", \"Not Informative Follow-Up\"],\n",
    "        \"Mean\": [np.mean(follow_up_scores[df_name][\"informative\"]), np.mean(follow_up_scores[df_name][\"not_informative\"])],\n",
    "        \"Variance\": [np.var(follow_up_scores[df_name][\"informative\"]), np.var(follow_up_scores[df_name][\"not_informative\"])]\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_stats = pd.DataFrame(data)\n",
    "    df_stats\n",
    "    print(df_stats)\n",
    "    return df\n",
    "\n",
    "models = [\"full\", \"gpt\", \"org\"]\n",
    "\n",
    "print(\"Mean/Var of values obtained from Human Labelling for \\nQuestions classified as Informative/Not Informative by GPT-4o\")\n",
    "\n",
    "for model in models:\n",
    "    print(model.upper())\n",
    "    df_stats = informative_score_per_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/Var of values obtained from Human Labelling for \n",
      "Questions classified as Informative/Not Informative by GPT-4o\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Informative Follow-Up</td>\n",
       "      <td>1.294643</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Informative Follow-Up</td>\n",
       "      <td>1.076115</td>\n",
       "      <td>1.062448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Category      Mean  Variance\n",
       "0      Informative Follow-Up  1.294643  1.011400\n",
       "1  Not Informative Follow-Up  1.076115  1.062448"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean/Var of values obtained from Human Labelling for \\nQuestions classified as Informative/Not Informative by GPT-4o\")\n",
    "data = {\n",
    "    \"Category\": [\"Informative Follow-Up\", \"Not Informative Follow-Up\"],\n",
    "    \"Mean\": [np.mean(informative_follow_up_scores), np.mean(not_informative_follow_up_scores)],\n",
    "    \"Variance\": [np.var(informative_follow_up_scores), np.var(not_informative_follow_up_scores)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_stats = pd.DataFrame(data)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: 3.2887\n",
      "P-Value: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Perform independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(informative_follow_up_scores, not_informative_follow_up_scores, equal_var=False)\n",
    "\n",
    "# Print results\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's d: 0.2146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_diff = np.mean(informative_follow_up_scores) - np.mean(not_informative_follow_up_scores)\n",
    "pooled_std = np.sqrt((np.var(informative_follow_up_scores) + np.var(not_informative_follow_up_scores)) / 2)\n",
    "cohen_d = mean_diff / pooled_std\n",
    "\n",
    "print(f\"Cohen's d: {cohen_d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
