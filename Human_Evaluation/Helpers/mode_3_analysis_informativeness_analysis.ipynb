{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(os.path.join(base_dir, \"Data_Augmentation\"))\n",
    "\n",
    "from llm import llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(gpt_df_with_answerability.loc[7, \"answer\"])\n",
    "# print(gpt_df_with_answerability.loc[7, \"complete_answer\"])\n",
    "# print(gpt_df_with_answerability.loc[7, \"generated_follow_up\"])\n",
    "# print(gpt_df_with_answerability.loc[7, \"generated_follow_up_answerability\"])\n",
    "full_df = pd.read_json('informativeness_output/full_df_with_answerability.json')\n",
    "org_df = pd.read_json('informativeness_output/org_df_with_answerability.json')\n",
    "gpt_df = pd.read_json('informativeness_output/gpt_df_with_answerability.json')\n",
    "\n",
    "ERROR_MSG = \"LLM failed to generate a response\"\n",
    "\n",
    "idx_with_incorrect_eval = []\n",
    "example_generated_follow_up = []\n",
    "\n",
    "def analyze_informativeness(df):\n",
    "    counter = Counter()\n",
    "    \n",
    "    for idx, data in df.iterrows():\n",
    "        follow_up_answerability = data[\"generated_follow_up_answerability\"].replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"').replace(\"'\", '\"')\n",
    "\n",
    "        if follow_up_answerability == ERROR_MSG: continue\n",
    "\n",
    "        # print(f\"idx: {data[\"id\"]} : {data[\"generated_follow_up_answerability\"]}\\n\")\n",
    "        nested_list = json.loads(follow_up_answerability)\n",
    "\n",
    "        if len(nested_list) != len(data[\"generated_follow_up\"]):\n",
    "            idx_with_incorrect_eval.append(idx)\n",
    "            example_generated_follow_up.append(data)\n",
    "\n",
    "        if ('Complete Answer','Original Answer') in nested_list:\n",
    "            print(idx)\n",
    "\n",
    "        counter.update(tuple(sublist) for sublist in nested_list)\n",
    "\n",
    "    return counter\n",
    "\n",
    "full_result = analyze_informativeness(full_df)\n",
    "org_result = analyze_informativeness(org_df)\n",
    "gpt_result = analyze_informativeness(gpt_df)\n",
    "print(idx_with_incorrect_eval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in example_generated_follow_up:\n",
    "#     print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../../Data_Augmentation/test_complete_answers.json\", \"r\") as f:\n",
    "# # Data_Augmentation/test.json \n",
    "# # with open(\"cleaned_train.json\", \"r\") as f:\n",
    "#     complete_answer_data = pd.DataFrame(json.load(f))\n",
    "\n",
    "# assert (len(full_df) == len(complete_answer_data))\n",
    "\n",
    "# df_with_errors = pd.DataFrame(example_generated_follow_up)\n",
    "# df_with_errors = df_with_errors.reset_index()\n",
    "# df_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Uncomment the code block below to run the api call again (it takes 2.5 Hours to run the entire dataset)\n",
    "# # Call GPT API for each model's (Full, Org, GPT) OA/CA/FUQs Pairs\n",
    "# index_raises_error = []\n",
    "# num_data = len(full_df)\n",
    "\n",
    "# ERROR_MESSAGE = 'LLM failed to generate a response'\n",
    "\n",
    "# def get_llm_response(i, df, ca_df):\n",
    "#     OA = df.loc[i, 'answer']\n",
    "#     CA = ca_df.loc[i, \"complete_answer\"]\n",
    "#     FUQ = df.loc[i, \"generated_follow_up\"]\n",
    "\n",
    "#     question = f\"Please tell me whether each of the questions in 'Follow Up Questions' is answerable by either/both the 'Original Answer' and the 'Complete Answer'. Original Answer: {OA}, Complete Answer: {CA}, Follow Up Questions: {FUQ}\"\n",
    "#     prompt = \"\"\"\n",
    "#         You must strictly follow these instructions:\n",
    "\n",
    "#         - Given an \"Original Answer\" and a \"Complete Answer,\" evaluate a list of \"Follow-Up Questions.\"\n",
    "#         - For each Follow-Up Question, determine whether it can be answered by:\n",
    "#             - The \"Original Answer\" only ‚Üí Return [\"Original Answer\"]\n",
    "#             - The \"Complete Answer\" only ‚Üí Return [\"Complete Answer\"]\n",
    "#             - Both answers ‚Üí Return [\"Original Answer\", \"Complete Answer\"]\n",
    "#             - Neither answer ‚Üí Return []\n",
    "\n",
    "#         ### **Criteria for Answerability (Assume Answerability Unless Completely Unrelated):**\n",
    "#         A Follow-Up Question (FUQ) is considered **answerable** if it contains **any relevant information** that connects to the answer in some way.  \n",
    "#         The answer does **not** need to be explicit, as long as it provides **some useful context** that **relates to** or **touches on** the FUQ's topic.\n",
    "\n",
    "#         1. **Direct Coverage** ‚úÖ  \n",
    "#         - The answer explicitly provides the required information.  \n",
    "#         - No inference is needed.  \n",
    "#         - Example:  \n",
    "#             - FUQ: \"What is blackbody radiation?\"  \n",
    "#             - OA: \"Blackbody radiation occurs when hot objects emit their own light.\"  \n",
    "#             - ‚úÖ Answerable by OA.  \n",
    "\n",
    "#         2. **Loose Inferability (Minimal Context Needed) ‚úÖ**  \n",
    "#         - The answer **does not directly answer** the FUQ but provides **some level of relevant context**, even if vague.  \n",
    "#         - If **some background knowledge** allows for a connection, it should be marked as answerable.  \n",
    "#         - Example:  \n",
    "#             - FUQ: \"How do blackbody radiation and white light differ in intensity?\"  \n",
    "#             - OA: \"Sufficiently hot objects produce their own light, a process called blackbody radiation.\"  \n",
    "#             - ‚úÖ Answerable by OA (since blackbody radiation involves emitted light, a connection to intensity can be made).  \n",
    "\n",
    "#         3. **Any Shared Concept Counts** ‚úÖ  \n",
    "#         - If the FUQ **mentions a topic** that appears in the OA or CA, it should be considered answerable.  \n",
    "#         - Even if the answer does **not fully explain** the FUQ, **any overlap** makes it answerable.  \n",
    "#         - Example:  \n",
    "#             - FUQ: \"What are some examples of high-energy emitting materials?\"  \n",
    "#             - OA: \"Torches, incandescent bulbs, and the sun emit light through blackbody radiation.\"  \n",
    "#             - ‚úÖ Answerable by OA (mention of blackbody radiation implies high-energy emission).  \n",
    "\n",
    "#         4. **Very Flexible Scope** ‚úÖ  \n",
    "#         - The FUQ **should be marked as answerable unless it is completely unrelated**.  \n",
    "#         - If **even a minor part** of the FUQ is **somewhat addressed**, it should be considered answerable.  \n",
    "#         - Example:  \n",
    "#             - FUQ: \"How does thermal decomposition contribute to the emission of white light?\"  \n",
    "#             - OA: \"Sufficiently hot objects produce their own light.\"  \n",
    "#             - ‚úÖ Answerable by OA (mention of heat and light emission allows for an inference).  \n",
    "\n",
    "#         5. **Extreme Leniency for Partial Answers** ‚úÖ  \n",
    "#         - A FUQ **does not have to be fully answered** to be considered answerable.  \n",
    "#         - Even if the answer **only hints at the topic** or **briefly addresses it**, it should be marked as answerable.  \n",
    "#         - Example:  \n",
    "#             - FUQ: \"How does the wavelength of light affect its energy?\"  \n",
    "#             - OA: \"Sufficiently hot objects emit their own light.\"  \n",
    "#             - ‚úÖ Answerable by OA (even though the full explanation isn't there, it touches on emitted light). \n",
    "\n",
    "#         ### **STRICT OUTPUT FORMAT RULES (DO NOT VIOLATE)**\n",
    "#         1. **The output must be a single-line string list.**  \n",
    "#         - Example output:  \n",
    "#             \"[[], [\"Complete Answer\"], [], [\"Original Answer\", \"Complete Answer\"], [\"Original Answer\"]]\"  \n",
    "#         - üö´ Do **not** return separate lines.  \n",
    "#         - üö´ Do **not** return explanations, summaries, or any extra text.  \n",
    "\n",
    "#         2. **Every Follow-Up Question must have exactly one corresponding entry.**  \n",
    "#         - The length of the output list **must match** the number of FUQs provided.  \n",
    "\n",
    "#         3. **Do NOT include escape characters ('\\') or newline characters ('\\n').**  \n",
    "#         - The output should be clean JSON without unnecessary formatting issues.  \n",
    "\n",
    "#         4. **The response must be a SINGLE list with elements separated by comma (',') wrapping all results.**  \n",
    "#         - üö´ Incorrect Example:  \n",
    "#             \"[\"Original Answer\", \"Complete Answer\"], [\"Original Answer\", \"Complete Answer\"], [\"Complete Answer\"], []\"\n",
    "#         - ‚úÖ Correct Example:  \n",
    "#             \"[[\"Original Answer\", \"Complete Answer\"], [\"Original Answer\", \"Complete Answer\"], [\"Complete Answer\"], []]\"\n",
    "\n",
    "#         5. **Do NOT add any extra text.**  \n",
    "#         - **ONLY return the structured list** and nothing else.  \n",
    "\n",
    "#         6. **POST-PROCESSING:**\n",
    "#         - **After generating the response, apply this replacement rule to ensure clean formatting:**  \n",
    "#             response = response.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"').replace(\"'\", '\"').replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "#         - This ensures the output is **valid JSON format** without incorrect quotation marks.\n",
    "\n",
    "#         ### **Input Data:**\n",
    "#         Original Answer: {OA}  \n",
    "#         Complete Answer: {CA}  \n",
    "#         Follow-Up Questions: {FUQ}  \n",
    "\n",
    "#         Now, analyze the given Follow-Up Questions and return ONLY the correctly structured list and nothing more:\n",
    "#     \"\"\"\n",
    "\n",
    "#     return llm_response(prompt, question)\n",
    "\n",
    "# def save_response_to_df(i, df, response):\n",
    "#     df.loc[i, \"generated_follow_up_answerability\"] = response\n",
    "\n",
    "# for i in range(len(df_with_errors)):\n",
    "# # for i in range(5):\n",
    "#     # Full Dataset\n",
    "#     response = get_llm_response(i, df_with_errors, complete_answer_data)\n",
    "\n",
    "#     if response == ERROR_MESSAGE:\n",
    "#         index_raises_error.append(i)\n",
    "\n",
    "#     save_response_to_df(i, df_with_errors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fixing rows that have incorrectly generated answerability data\n",
    "# for row in df_with_errors.iterrows():\n",
    "#     print(row[1].generated_follow_up)\n",
    "#     print(row[1].generated_follow_up_answerability.replace(\"\\\"\", '\\''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Complete Answer',): 712,\n",
       "         (): 676,\n",
       "         ('Original Answer', 'Complete Answer'): 397,\n",
       "         ('Original Answer',): 255})"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Original Answer', 'Complete Answer'): 610,\n",
       "         (): 592,\n",
       "         ('Original Answer',): 582,\n",
       "         ('Complete Answer',): 542,\n",
       "         ('Complete Answer', 'Original Answer'): 1})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Original Answer', 'Complete Answer'): 611,\n",
       "         (): 592,\n",
       "         ('Original Answer',): 582,\n",
       "         ('Complete Answer',): 542})"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the count of ('Complete Answer', 'Original Answer') to ('Original Answer', 'Complete Answer')\n",
    "org_result[('Original Answer', 'Complete Answer')] += org_result[('Complete Answer', 'Original Answer')]\n",
    "\n",
    "# Remove the ('Complete Answer', 'Original Answer') entry\n",
    "del org_result[('Complete Answer', 'Original Answer')]\n",
    "\n",
    "org_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Complete Answer',): 674,\n",
       "         (): 624,\n",
       "         ('Original Answer', 'Complete Answer'): 400,\n",
       "         ('Original Answer',): 179})"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_table = pd.DataFrame([\n",
    "    [\"Generated FollowupQ\", \"Answered by CA\", \"Unanswered by CA\"],\n",
    "    [\"Answered by OA\", \"No new information\", \"Inappropriate CA\"],\n",
    "    [\"Unanswered by OA\", \"New Information\", \"Unrelated followupQ\"]\n",
    "])\n",
    "\n",
    "def display_result(df, table):\n",
    "    count = 0\n",
    "\n",
    "    for value in df.values():\n",
    "        count += value\n",
    "\n",
    "    for key, value in df.items():\n",
    "        percentage = round((value / count), 2)\n",
    "        match key:\n",
    "            case ():\n",
    "                table.iloc[2,2] = percentage\n",
    "            case ('Complete Answer',):\n",
    "                table.iloc[2,1] = percentage\n",
    "            case ('Original Answer',):\n",
    "                table.iloc[1,2] = percentage\n",
    "            case ('Original Answer', 'Complete Answer'):\n",
    "                table.iloc[1,1] = percentage\n",
    "    \n",
    "    return table\n",
    "\n",
    "full_result_table = display_result(full_result, template_table.copy())\n",
    "org_result_table = display_result(org_result, template_table.copy())\n",
    "gpt_result_table = display_result(gpt_result, template_table.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generated FollowupQ</td>\n",
       "      <td>Answered by CA</td>\n",
       "      <td>Unanswered by CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answered by OA</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unanswered by OA</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0               1                 2\n",
       "0  Generated FollowupQ  Answered by CA  Unanswered by CA\n",
       "1       Answered by OA            0.19              0.12\n",
       "2     Unanswered by OA            0.35              0.33"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generated FollowupQ</td>\n",
       "      <td>Answered by CA</td>\n",
       "      <td>Unanswered by CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answered by OA</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unanswered by OA</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0               1                 2\n",
       "0  Generated FollowupQ  Answered by CA  Unanswered by CA\n",
       "1       Answered by OA            0.26              0.25\n",
       "2     Unanswered by OA            0.23              0.25"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generated FollowupQ</td>\n",
       "      <td>Answered by CA</td>\n",
       "      <td>Unanswered by CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answered by OA</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unanswered by OA</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0               1                 2\n",
       "0  Generated FollowupQ  Answered by CA  Unanswered by CA\n",
       "1       Answered by OA            0.21               0.1\n",
       "2     Unanswered by OA            0.36              0.33"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
