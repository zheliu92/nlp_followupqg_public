{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data size of 934 reduced to -> 155\n"
     ]
    }
   ],
   "source": [
    "### Human Evaluator Verified Dataset\n",
    "data = pd.read_csv(\"output_mode_2_only_valid_questions.csv\")\n",
    "og_data_size = len(data)\n",
    "\n",
    "df_gp = data[[\"group\", \"prefix\"]]\n",
    "df_gp = df_gp.drop_duplicates(subset=[\"prefix\"])\n",
    "print(f\"Original Data size of {og_data_size} reduced to -> {len(df_gp)}\")\n",
    "\n",
    "# group by the first part of the prefix - e.g 3000\n",
    "df_gp['prefix'], df_gp['index'] = zip(*df_gp['prefix'].str.split('_').apply(lambda x: (x[0], x[1])))\n",
    "df_gp['index'] = df_gp['index'].astype(int) - 1\n",
    "df_gp['group'] = df_gp['group'].str.split('_').str[0]\n",
    "df_gp = df_gp.groupby(['group', 'prefix'])['index'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Human Evaluator Verified Dataset\n",
    "# algorithm to parse through each group and every prefix to collect a list of OQ/OA/CA/FUQS\n",
    "full_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/full_clustered.json'\n",
    "gpt_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/gpt_clustered.json'\n",
    "org_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/org_clustered.json'\n",
    "\n",
    "# Load data\n",
    "full_df = pd.read_json(full_path)\n",
    "gpt_df = pd.read_json(gpt_path)\n",
    "org_df = pd.read_json(org_path)\n",
    "\n",
    "filtered_data_full = pd.DataFrame(columns=full_df.columns)\n",
    "filtered_data_gpt = pd.DataFrame(columns=gpt_df.columns)\n",
    "filtered_data_org = pd.DataFrame(columns=org_df.columns)\n",
    "\n",
    "json_df = None\n",
    "\n",
    "# Explode the `generated_follow_up` column\n",
    "# json_data = json_data.explode('generated_follow_up', ignore_index=True)\n",
    "\n",
    "for index, row in df_gp.iterrows():\n",
    "    match row['group']:\n",
    "        case 'full':\n",
    "            json_df = full_df\n",
    "        case 'gpt':\n",
    "            print()\n",
    "            json_df = gpt_df\n",
    "        case 'org':\n",
    "            json_df = org_df\n",
    "        case _:\n",
    "            print(\"Invalid File Found\")\n",
    "            json_df = None\n",
    "            break\n",
    "    \n",
    "    for _, json_data in json_df.iterrows():\n",
    "        if int(json_data['id']) != int(row['prefix']):\n",
    "            continue\n",
    "        \n",
    "        relevant_follow_ups = np.array(json_data['generated_follow_up'])\n",
    "        relevant_follow_ups = relevant_follow_ups[row['index']]\n",
    "        row_data = json_data\n",
    "        row_data['generated_follow_up'] = relevant_follow_ups\n",
    "\n",
    "        match row['group']:\n",
    "            case 'full':\n",
    "                filtered_data_full.loc[len(filtered_data_full)] = row_data\n",
    "            case 'gpt':\n",
    "                filtered_data_gpt.loc[len(filtered_data_gpt)] = row_data\n",
    "            case 'org':\n",
    "                filtered_data_org.loc[len(filtered_data_org)] = row_data\n",
    "            case _:\n",
    "                print(\"Invalid File Found\")\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entire Dataset Collected by Each of the 3 Models\n",
    "full_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/full_clustered.json'\n",
    "gpt_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/gpt_clustered.json'\n",
    "org_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/org_clustered.json'\n",
    "\n",
    "# Load data\n",
    "full_df = pd.read_json(full_path)\n",
    "gpt_df = pd.read_json(gpt_path)\n",
    "org_df = pd.read_json(org_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL model average number of follow-ups: 4.11377245508982\n",
      "ORG model average number of follow-ups: 4.688622754491018\n",
      "GPT model average number of follow-ups: 3.7824351297405188\n"
     ]
    }
   ],
   "source": [
    "print(f\"FULL model average number of follow-ups: {full_df['generated_follow_up'].apply(lambda x : len(x)).mean()}\")\n",
    "print(f\"ORG model average number of follow-ups: {org_df['generated_follow_up'].apply(lambda x : len(x)).mean()}\")\n",
    "print(f\"GPT model average number of follow-ups: {gpt_df['generated_follow_up'].apply(lambda x : len(x)).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [What are the primary sources of heat for anim...\n",
       "1    [What are the potential long-term effects of f...\n",
       "2    [What are some examples of the side conflicts ...\n",
       "3    [What is the role of the human ear in interpre...\n",
       "4    [Can you explain what \"sandbox game\" means in ...\n",
       "Name: generated_follow_up, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[\"generated_follow_up\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_distinct_n(df, n):\n",
    "    distinct_dfs = []\n",
    "    for follow_ups in df[\"generated_follow_up\"]:\n",
    "        unique_n_grams = set()\n",
    "        ngrams = []\n",
    "        for follow_up in follow_ups:\n",
    "            words = follow_up.split()\n",
    "            if len(words) < n:\n",
    "                return 0 \n",
    "    \n",
    "            ngrams.extend([tuple(words[i:i+n]) for i in range(len(words) - n + 1)])\n",
    "            unique_n_grams.update(ngrams)\n",
    "    \n",
    "        distinct_dfs.append((len(unique_n_grams) / len(ngrams)) * 100)\n",
    "    \n",
    "    return distinct_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distinct-1 (%) for Full Model: 77.09446487061521\n",
      "Average Distinct-1 (%) for ORG Model: 66.06013286221676\n",
      "Average Distinct-1 (%) for GPT Model: 77.36019186050663\n",
      "Average Distinct-2 (%) for Full Model: 94.85221978683695\n",
      "Average Distinct-2 (%) for ORG Model: 91.11533928099017\n",
      "Average Distinct-2 (%) for GPT Model: 94.41104283452837\n"
     ]
    }
   ],
   "source": [
    "# 1-distinct df \n",
    "n = 1\n",
    "full_distinct_dfs_1 = find_average_distinct_n(full_df, n)\n",
    "org_distinct_dfs_1 = find_average_distinct_n(org_df, n)\n",
    "gpt_distinct_dfs_1 = find_average_distinct_n(gpt_df, n)\n",
    "\n",
    "print(f\"Average Distinct-1 (%) for Full Model: {np.mean(full_distinct_dfs_1)}\")\n",
    "print(f\"Average Distinct-1 (%) for ORG Model: {np.mean(org_distinct_dfs_1)}\")\n",
    "print(f\"Average Distinct-1 (%) for GPT Model: {np.mean(gpt_distinct_dfs_1)}\")\n",
    "\n",
    "n = 2\n",
    "full_distinct_dfs_2 = find_average_distinct_n(full_df, n)\n",
    "org_distinct_dfs_2 = find_average_distinct_n(org_df, n)\n",
    "gpt_distinct_dfs_2 = find_average_distinct_n(gpt_df, n)\n",
    "\n",
    "print(f\"Average Distinct-2 (%) for Full Model: {np.mean(full_distinct_dfs_2)}\")\n",
    "print(f\"Average Distinct-2 (%) for ORG Model: {np.mean(org_distinct_dfs_2)}\")\n",
    "print(f\"Average Distinct-2 (%) for GPT Model: {np.mean(gpt_distinct_dfs_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tkang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tkang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Methods to Determine if a Sentence is a valid question\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a regex pattern to match informative questions\n",
    "invalid_words_pattern = r'<\\w+>'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def contains_question_mark(sentence, invalid_questions):\n",
    "    if sentence[-1] == '?':\n",
    "        return True\n",
    "    else:\n",
    "        invalid_questions.append(sentence)\n",
    "        return False\n",
    "\n",
    "def is_question_dependency_parsing(sentence, invalid_questions):\n",
    "    \"\"\"Detects whether a sentence is a question using dependency parsing.\"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Track question indicators\n",
    "    is_wh_question = False\n",
    "    is_aux_question = False\n",
    "    \n",
    "    # ✅ Track negation but don't invalidate questions outright\n",
    "    # negation_found = any(token.dep_ == \"neg\" for token in doc)\n",
    "\n",
    "    for token in doc:\n",
    "        # ✅ WH-Questions (What, Why, How, Where, etc.)\n",
    "        if token.dep_ in {\"attr\", \"nsubj\", \"advmod\"} and token.head.dep_ in {\"ROOT\", \"nsubj\", \"advmod\"}:\n",
    "            is_wh_question = True\n",
    "        \n",
    "        # ✅ Yes/No Questions (Do you..., Can we..., Is it...)\n",
    "        if token.dep_ == \"aux\" and token.head.dep_ == \"ROOT\":\n",
    "            is_aux_question = True\n",
    "        \n",
    "        # ✅ Special Case: \"Why\", \"How\", \"Where\" directly at the start are always questions\n",
    "        if token.text.lower() in {\"why\", \"how\", \"where\"}:\n",
    "            is_wh_question = True\n",
    "    \n",
    "    # ✅ Final Decision:\n",
    "    if not (is_wh_question or is_aux_question):\n",
    "        # 🔥 **NEW: Only invalidate if negation makes it rhetorical**  \n",
    "        invalid_questions.append(sentence)\n",
    "        return False  # Negation in non-WH questions is more likely rhetorical\n",
    "    # ❌ Not a valid question\n",
    "    return True\n",
    "\n",
    "def contains_invalid_word(question, invalid_questions):\n",
    "    if bool(re.search(invalid_words_pattern, question)):\n",
    "        invalid_questions.append(question)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_consecutive_word_sequences(sentence):\n",
    "    \"\"\"Extracts consecutive word sequences of at least `min_length` words.\"\"\"\n",
    "    # min_length = int(len(sentence.split(\" \"))/3) # if a sentence uses over half of it's words, it's copying\n",
    "    min_length = 10\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)  # Extract words\n",
    "    sequences = set()\n",
    "\n",
    "    for i in range(len(words) - min_length + 1):\n",
    "        phrase = \" \".join(words[i:i + min_length])  # Create word sequence\n",
    "        sequences.add(phrase)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def contains_duplicate_words(og_question, og_answer, follow_up_question, invalid_questions):\n",
    "    \"\"\"Checks if there is a common substring of at least `min_length` consecutive words between two sentences.\"\"\"\n",
    "    og_question_answer = og_question + \" \" + og_answer  # Merge question and answer\n",
    "    \n",
    "    og_seq = get_consecutive_word_sequences(og_question_answer)\n",
    "    follow_up_seq = get_consecutive_word_sequences(follow_up_question)\n",
    "\n",
    "    common_sequences = og_seq & follow_up_seq  # Only allow consecutive matches\n",
    "\n",
    "    if common_sequences:\n",
    "        invalid_questions.append(follow_up_question)\n",
    "        return True  # Found duplicate consecutive words\n",
    "    return False\n",
    "\n",
    "# combining all the other methods\n",
    "def is_valid_question(question, og_question, og_answer, invalid_questions):\n",
    "    # print(question)\n",
    "    return (\n",
    "        contains_question_mark(question, invalid_questions) and \n",
    "        is_question_dependency_parsing(question, invalid_questions) and \n",
    "        not contains_invalid_word(question, invalid_questions) and\n",
    "        not contains_duplicate_words(og_question, og_answer, question, invalid_questions)\n",
    "        )\n",
    "\n",
    "def filterInvalidFollowUpQuestions(df):\n",
    "    # df columns = ['id', 'question', 'answer', 'follow-up', 'relation', 'generated_follow_up']\n",
    "    invalid_questions = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        original_question = row['question']\n",
    "        original_answer = row['answer']\n",
    "\n",
    "        valid_questions = [follow_up for follow_up in row['generated_follow_up'] if is_valid_question(follow_up, original_question, original_answer, invalid_questions)]\n",
    "        df.at[index, \"generated_follow_up\"] = valid_questions\n",
    "    \n",
    "    return df, invalid_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_question = \"ELI5: What is a heat dome?\"\n",
    "# og_answer = \"Is when the high pressure in the atmosphere traps the hot air in below. As you would know, the hot air rises, which only causes the air to compress because of the pressure from above and it gets hotter, hotter, hotter and denser (That's why you would kill for a glass of water, the hot air is literally pushing you against more hot air)\"\n",
    "# question = \">That's why you would kill for a glass of water  Is that why people die from heatstroke?\"\n",
    "\n",
    "# contains_duplicate_words(og_question, og_answer, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_json(\"full_df.json\", orient=\"records\", indent=4)\n",
    "gpt_df.to_json(\"gpt_df.json\", orient=\"records\", indent=4)\n",
    "org_df.to_json(\"org_df.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out all invalid follow up questions in FULL: 2061\n",
      "after filtering out all invalid follow up questions in FULL: 1931\n",
      "before filtering out all invalid follow up questions in GPT: 1895\n",
      "after filtering out all invalid follow up questions in GPT: 1827\n",
      "before filtering out all invalid follow up questions in ORG: 2349\n",
      "after filtering out all invalid follow up questions in ORG: 1568\n"
     ]
    }
   ],
   "source": [
    "print(f\"before filtering out all invalid follow up questions in FULL: { len(full_df['generated_follow_up'].explode()) }\")\n",
    "full_df_valid_follow_up_only, full_invalid_questions = filterInvalidFollowUpQuestions(full_df.copy())\n",
    "print(f\"after filtering out all invalid follow up questions in FULL: {len(full_df_valid_follow_up_only['generated_follow_up'].explode())}\")\n",
    "\n",
    "print(f\"before filtering out all invalid follow up questions in GPT: {len(gpt_df['generated_follow_up'].explode())}\")\n",
    "gpt_df_valid_follow_up_only, gpt_invalid_questions = filterInvalidFollowUpQuestions(gpt_df.copy())\n",
    "print(f\"after filtering out all invalid follow up questions in GPT: {len(gpt_df_valid_follow_up_only['generated_follow_up'].explode())}\")\n",
    "\n",
    "print(f\"before filtering out all invalid follow up questions in ORG: {len(org_df['generated_follow_up'].explode())}\")\n",
    "org_df_valid_follow_up_only, org_invalid_questions = filterInvalidFollowUpQuestions(org_df.copy())\n",
    "print(f\"after filtering out all invalid follow up questions in ORG: {len(org_df_valid_follow_up_only['generated_follow_up'].explode())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_valid_follow_up_only.to_json(\"diversity_output/full_valid_fq_only.json\", orient=\"records\", indent=4)\n",
    "gpt_df_valid_follow_up_only.to_json(\"diversity_output/gpt_valid_fq_only.json\", orient=\"records\", indent=4)\n",
    "org_df_valid_follow_up_only.to_json(\"diversity_output/org_valid_fq_only.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tkang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/tkang/miniforge3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start working on model\n",
      "Evaluating distance threshold: 0.3...\n",
      "Task 3034 - Only 1 sample, skipping clustering.\n",
      "Task 3094 - Only 1 sample, skipping clustering.\n",
      "Task 3452 - Only 1 sample, skipping clustering.\n",
      "Start working on model\n",
      "Evaluating distance threshold: 0.3...\n",
      "Task 3032 - Only 1 sample, skipping clustering.\n",
      "Task 3219 - Only 1 sample, skipping clustering.\n",
      "Task 3311 - Only 1 sample, skipping clustering.\n",
      "Task 3326 - Only 1 sample, skipping clustering.\n",
      "Task 3359 - Only 1 sample, skipping clustering.\n",
      "Task 3393 - Only 1 sample, skipping clustering.\n",
      "Task 3410 - Only 1 sample, skipping clustering.\n",
      "Start working on model\n",
      "Evaluating distance threshold: 0.3...\n",
      "Task 3002 - Only 1 sample, skipping clustering.\n",
      "Task 3010 - Only 1 sample, skipping clustering.\n",
      "Task 3012 - Only 1 sample, skipping clustering.\n",
      "Task 3029 - Only 1 sample, skipping clustering.\n",
      "Task 3034 - Only 1 sample, skipping clustering.\n",
      "Task 3036 - Only 1 sample, skipping clustering.\n",
      "Task 3059 - Only 1 sample, skipping clustering.\n",
      "Task 3074 - Only 1 sample, skipping clustering.\n",
      "Task 3082 - Only 1 sample, skipping clustering.\n",
      "Task 3087 - Only 1 sample, skipping clustering.\n",
      "Task 3115 - Only 1 sample, skipping clustering.\n",
      "Task 3117 - Only 1 sample, skipping clustering.\n",
      "Task 3127 - Only 1 sample, skipping clustering.\n",
      "Task 3129 - Only 1 sample, skipping clustering.\n",
      "Task 3152 - Only 1 sample, skipping clustering.\n",
      "Task 3172 - Only 1 sample, skipping clustering.\n",
      "Task 3173 - Only 1 sample, skipping clustering.\n",
      "Task 3180 - Only 1 sample, skipping clustering.\n",
      "Task 3240 - Only 1 sample, skipping clustering.\n",
      "Task 3247 - Only 1 sample, skipping clustering.\n",
      "Task 3249 - Only 1 sample, skipping clustering.\n",
      "Task 3260 - Only 1 sample, skipping clustering.\n",
      "Task 3261 - Only 1 sample, skipping clustering.\n",
      "Task 3262 - Only 1 sample, skipping clustering.\n",
      "Task 3279 - Only 1 sample, skipping clustering.\n",
      "Task 3285 - Only 1 sample, skipping clustering.\n",
      "Task 3286 - Only 1 sample, skipping clustering.\n",
      "Task 3287 - Only 1 sample, skipping clustering.\n",
      "Task 3303 - Only 1 sample, skipping clustering.\n",
      "Task 3305 - Only 1 sample, skipping clustering.\n",
      "Task 3312 - Only 1 sample, skipping clustering.\n",
      "Task 3376 - Only 1 sample, skipping clustering.\n",
      "Task 3410 - Only 1 sample, skipping clustering.\n",
      "Task 3430 - Only 1 sample, skipping clustering.\n",
      "Task 3431 - Only 1 sample, skipping clustering.\n",
      "Task 3461 - Only 1 sample, skipping clustering.\n",
      "Task 3471 - Only 1 sample, skipping clustering.\n",
      "Task 3476 - Only 1 sample, skipping clustering.\n",
      "Task 3477 - Only 1 sample, skipping clustering.\n",
      "Task 3482 - Only 1 sample, skipping clustering.\n",
      "Clustering complete for all models.\n"
     ]
    }
   ],
   "source": [
    "### cluster by similar follow up questions\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "models = [full_df_valid_follow_up_only, gpt_df_valid_follow_up_only, org_df_valid_follow_up_only]\n",
    "model_names = [\"full\", \"gpt\", \"org\"]\n",
    "# distance_thresholds = np.arange(0, 1.1, 0.1)\n",
    "distance_thresholds = [0.3]\n",
    "clustered_models = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    words = word_tokenize(sentence)  # Tokenize sentence into words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(filtered_words)  # Reconstruct sentence\n",
    "\n",
    "for idx in range(len(models)):\n",
    "    print(\"Start working on model\")\n",
    "    model_name = model_names[idx]\n",
    "    data = models[idx].copy()\n",
    "\n",
    "    # Loop over all rows in the dataset\n",
    "    for dt in distance_thresholds:\n",
    "        print(f\"Evaluating distance threshold: {dt}...\")\n",
    "        # Store clustered follow-up questions\n",
    "        clustered_follow_ups = []\n",
    "        clustered_follow_ups_count = []\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            task_id = row['id']\n",
    "            \n",
    "            # remove stopwrods\n",
    "            current_corpus = [remove_stopwords(sentence) for sentence in row[\"generated_follow_up\"]]\n",
    "\n",
    "            # Ensure it's a list (not a single string)\n",
    "            if isinstance(current_corpus, str):\n",
    "                current_corpus = [current_corpus]\n",
    "            elif not isinstance(current_corpus, list):\n",
    "                clustered_follow_ups.append([])\n",
    "                clustered_follow_ups_count.append(0)\n",
    "                continue\n",
    "\n",
    "            if len(current_corpus) <= 1:\n",
    "                clustered_follow_ups.append(current_corpus)  # Keep as is\n",
    "                clustered_follow_ups_count.append(1)  # Single item cluster\n",
    "                print(f\"Task {task_id} - Only 1 sample, skipping clustering.\")\n",
    "                continue\n",
    "\n",
    "            # Convert sentences to embeddings\n",
    "            corpus_embeddings = embedder.encode(current_corpus)\n",
    "\n",
    "            # Perform agglomerative clustering\n",
    "            clustering_model = AgglomerativeClustering(\n",
    "                metric='cosine', linkage='average',\n",
    "                n_clusters=None, distance_threshold=dt\n",
    "            ) \n",
    "            clustering_model.fit(corpus_embeddings)\n",
    "            cluster_assignment = clustering_model.labels_\n",
    "\n",
    "            # Organize clustered sentences\n",
    "            clustered_sentences = {}\n",
    "            for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "                if cluster_id not in clustered_sentences:\n",
    "                    clustered_sentences[cluster_id] = []\n",
    "                clustered_sentences[cluster_id].append(current_corpus[sentence_id])\n",
    "\n",
    "            # Collect all clustered follow-ups\n",
    "            clustered_list = list(clustered_sentences.values())\n",
    "\n",
    "            # Append clustered results to columns\n",
    "            clustered_follow_ups.append(clustered_list)\n",
    "            clustered_follow_ups_count.append(len(clustered_list))\n",
    "\n",
    "        # Add new columns to DataFrame\n",
    "        data[f\"clustered_dt_{dt}\"] = clustered_follow_ups\n",
    "        data[f\"clustered_count_dt_{dt}\"] = clustered_follow_ups_count\n",
    "\n",
    "    clustered_models.append(data)\n",
    "\n",
    "print(\"Clustering complete for all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered_full = pd.DataFrame(clustered_models[0])\n",
    "df_clustered_gpt = pd.DataFrame(clustered_models[1])\n",
    "df_clustered_org = pd.DataFrame(clustered_models[2])\n",
    "average_cluster_count_per_dt = {\n",
    "    \"full\": [],\n",
    "    \"gpt\": [],\n",
    "    \"org\": []\n",
    "}\n",
    "\n",
    "for model_i in range(3):\n",
    "    l = []\n",
    "    for i in distance_thresholds:\n",
    "        l.append(clustered_models[model_i][f\"clustered_count_dt_{i}\"].mean())\n",
    "    average_cluster_count_per_dt[model_names[model_i]] = l\n",
    "\n",
    "# df_clustered_full[\"generated_follow_ups_clustered_count\"]\n",
    "\n",
    "# temp = df_clustered_full[df_clustered_full[\"generated_follow_ups_clustered_count\"] > 1]\n",
    "# for group in temp[\"generated_follow_ups_clustered\"]:\n",
    "#     print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full': [3.5668662674650697],\n",
       " 'gpt': [3.241516966067864],\n",
       " 'org': [3.057884231536926]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_cluster_count_per_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cluster Count for Distance Threshold (0.3):\n",
      "Model full: 3.5668662674650697\n",
      "Model gpt: 3.241516966067864\n",
      "Model org: 3.057884231536926\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(f\"Average Cluster Count for Distance Threshold ({distance_thresholds[i]}):\")\n",
    "    for model, values in average_cluster_count_per_dt.items():\n",
    "        print(f\"Model {model}: {values[i]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cluster Count across all Distance Threshold\n",
      "Model FULL: 3.5668662674650697\n",
      "Model ORG: 3.057884231536926\n",
      "Model GPT: 3.241516966067864\n"
     ]
    }
   ],
   "source": [
    "### after removing stopwords and using min_length = 10\n",
    "print(f\"Average Cluster Count across all Distance Threshold\")\n",
    "print(f\"Model FULL: {np.mean(average_cluster_count_per_dt['full'])}\")\n",
    "print(f\"Model ORG: {np.mean(average_cluster_count_per_dt['org'])}\")\n",
    "print(f\"Model GPT: {np.mean(average_cluster_count_per_dt['gpt'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cluster Count across all Distance Threshold\n",
      "Model FULL: 2.378697151152241\n",
      "Model ORG: 2.343131917982218\n",
      "Model GPT: 2.195064416621303\n"
     ]
    }
   ],
   "source": [
    "### ^ check the second one above\n",
    "print(f\"Average Cluster Count across all Distance Threshold\")\n",
    "print(f\"Model FULL: {np.mean(average_cluster_count_per_dt['full'])}\")\n",
    "print(f\"Model ORG: {np.mean(average_cluster_count_per_dt['org'])}\")\n",
    "print(f\"Model GPT: {np.mean(average_cluster_count_per_dt['gpt'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cluster Count across all Distance Threshold\n",
      "Model FULL: 2.4071856287425146\n",
      "Model ORG: 2.6162221012520415\n",
      "Model GPT: 2.1981491562329887\n"
     ]
    }
   ],
   "source": [
    "### after removing stopwords - and using 1/3 length as min_length\n",
    "print(f\"Average Cluster Count across all Distance Threshold\")\n",
    "print(f\"Model FULL: {np.mean(average_cluster_count_per_dt['full'])}\")\n",
    "print(f\"Model ORG: {np.mean(average_cluster_count_per_dt['org'])}\")\n",
    "print(f\"Model GPT: {np.mean(average_cluster_count_per_dt['gpt'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cluster Count across all Distance Threshold\n",
      "Model FULL: 2.4289602612955905\n",
      "Model ORG: 2.3124659771366356\n",
      "Model GPT: 2.256033387769915\n"
     ]
    }
   ],
   "source": [
    "# for when min_length = 10 for get_consecutive_word_sequences function\n",
    "print(f\"Average Cluster Count across all Distance Threshold\")\n",
    "print(f\"Model FULL: {np.mean(average_cluster_count_per_dt['full'])}\")\n",
    "print(f\"Model ORG: {np.mean(average_cluster_count_per_dt['org'])}\")\n",
    "print(f\"Model GPT: {np.mean(average_cluster_count_per_dt['gpt'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'generated_follow_ups_clustered_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'generated_follow_ups_clustered_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate the average number of clusters for each OQ/OA pair\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_cluster_count_full_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_clustered_full\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_follow_ups_clustered_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m mean_cluster_count_org_df \u001b[38;5;241m=\u001b[39m df_clustered_org[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_follow_ups_clustered_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      4\u001b[0m mean_cluster_count_gpt_df \u001b[38;5;241m=\u001b[39m df_clustered_gpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_follow_ups_clustered_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'generated_follow_ups_clustered_count'"
     ]
    }
   ],
   "source": [
    "# Calculate the average number of clusters for each OQ/OA pair\n",
    "mean_cluster_count_full_df = df_clustered_full[\"generated_follow_ups_clustered_count\"].mean()\n",
    "mean_cluster_count_org_df = df_clustered_org[\"generated_follow_ups_clustered_count\"].mean()\n",
    "mean_cluster_count_gpt_df = df_clustered_gpt[\"generated_follow_ups_clustered_count\"].mean()\n",
    "\n",
    "print(f\"Average Number of Unique Follow Up Questions (Clusters) for Full Dataset: {mean_cluster_count_full_df}\")\n",
    "print(f\"Average Number of Unique Follow Up Questions (Clusters) for ORG Dataset: {mean_cluster_count_org_df}\")\n",
    "print(f\"Average Number of Unique Follow Up Questions (Clusters) for GPT Dataset: {mean_cluster_count_gpt_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
