{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>prefix</th>\n",
       "      <th>newInformation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3000_4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full_1</td>\n",
       "      <td>3001_1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>org_10</td>\n",
       "      <td>3099_7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       group  prefix  newInformation\n",
       "0     full_1  3000_1               0\n",
       "1     full_1  3000_2               2\n",
       "2     full_1  3000_3               2\n",
       "3     full_1  3000_4               2\n",
       "4     full_1  3001_1               2\n",
       "...      ...     ...             ...\n",
       "1123  org_10  3099_3               0\n",
       "1124  org_10  3099_4               0\n",
       "1125  org_10  3099_5               0\n",
       "1126  org_10  3099_6               1\n",
       "1127  org_10  3099_7               2\n",
       "\n",
       "[1128 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human Evaluation Data\n",
    "df_human = pd.read_csv(\"human_eval_all_questions_annotated.csv\") # only valid FUQs\n",
    "df_new_info_human = df_human[[\"group\", \"prefix\", \"newInformation\"]].copy()\n",
    "df_new_info_human[\"group\"] = df_new_info_human[\"group\"].astype(pd.StringDtype())\n",
    "df_new_info_human[\"prefix\"] = df_new_info_human[\"prefix\"].astype(pd.StringDtype())\n",
    "df_new_info_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1128 entries, 0 to 1127\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   group           1128 non-null   string\n",
      " 1   prefix          1128 non-null   string\n",
      " 2   newInformation  1128 non-null   int64 \n",
      "dtypes: int64(1), string(2)\n",
      "memory usage: 26.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_new_info_human.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Evaluation Data\n",
    "dfs_auto = {\n",
    "    \"full\": pd.read_json('informativeness_output/full_df_with_answerability.json'),\n",
    "    \"org\": pd.read_json('informativeness_output/org_df_with_answerability.json'),\n",
    "    \"gpt\": pd.read_json('informativeness_output/gpt_df_with_answerability.json')\n",
    "}\n",
    "\n",
    "ERROR_MSG = \"LLM failed to generate a response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>follow-up</th>\n",
       "      <th>relation</th>\n",
       "      <th>generated_follow_up</th>\n",
       "      <th>complete_answer</th>\n",
       "      <th>generated_follow_up_answerability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>ELI5 Do animals tan?</td>\n",
       "      <td>Animals can get sunburned like we do, pigs for...</td>\n",
       "      <td>But can they tan? Does their body create color...</td>\n",
       "      <td>Related</td>\n",
       "      <td>[What are the primary sources of heat for anim...</td>\n",
       "      <td>Yes, some animals can tan. Similar to humans, ...</td>\n",
       "      <td>[[], [\"Complete Answer\"], [\"Complete Answer\"],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3001</td>\n",
       "      <td>ELI5 What is GERD?</td>\n",
       "      <td>Gerd (GastroEsophageal Reflux Disease) is a pr...</td>\n",
       "      <td>I c this was the answer I was looking for. I m...</td>\n",
       "      <td>Related</td>\n",
       "      <td>[What are the potential long-term effects of f...</td>\n",
       "      <td>GERD, or Gastroesophageal Reflux Disease, is a...</td>\n",
       "      <td>[[“Original Answer”, “Complete Answer”], [], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3002</td>\n",
       "      <td>eli5 Why didn’t the dwarves fight in the War o...</td>\n",
       "      <td>They did, the movies skip a lot of side detail...</td>\n",
       "      <td>Thanks. I’m listening to the audiobooks and to...</td>\n",
       "      <td>Related</td>\n",
       "      <td>[What are some examples of the side conflicts ...</td>\n",
       "      <td>The dwarves did not actively participate in th...</td>\n",
       "      <td>[[“Original Answer”], [“Complete Answer”], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3003</td>\n",
       "      <td>ELI5: How does our voice turn into code / get ...</td>\n",
       "      <td>In a nutshell:  The sound waves from your voic...</td>\n",
       "      <td>Hey, I totally respect and appreciate this. Bu...</td>\n",
       "      <td>Related</td>\n",
       "      <td>[What is the role of the human ear in interpre...</td>\n",
       "      <td>When you speak into a phone, your voice genera...</td>\n",
       "      <td>[[], [\"Original Answer\", \"Complete Answer\"], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3004</td>\n",
       "      <td>eli5 how is the basic of minecraft. game....li...</td>\n",
       "      <td>The endgame is to get to \"the end\" where there...</td>\n",
       "      <td>Wasn't the End dimension only added to minecra...</td>\n",
       "      <td>Slightly Related</td>\n",
       "      <td>[Can you explain what \"sandbox game\" means in ...</td>\n",
       "      <td>Minecraft is an open-world sandbox game where ...</td>\n",
       "      <td>[[\"Original Answer\", \"Complete Answer\"], [\"Com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  3000                               ELI5 Do animals tan?   \n",
       "1  3001                                 ELI5 What is GERD?   \n",
       "2  3002  eli5 Why didn’t the dwarves fight in the War o...   \n",
       "3  3003  ELI5: How does our voice turn into code / get ...   \n",
       "4  3004  eli5 how is the basic of minecraft. game....li...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Animals can get sunburned like we do, pigs for...   \n",
       "1  Gerd (GastroEsophageal Reflux Disease) is a pr...   \n",
       "2  They did, the movies skip a lot of side detail...   \n",
       "3  In a nutshell:  The sound waves from your voic...   \n",
       "4  The endgame is to get to \"the end\" where there...   \n",
       "\n",
       "                                           follow-up          relation  \\\n",
       "0  But can they tan? Does their body create color...           Related   \n",
       "1  I c this was the answer I was looking for. I m...           Related   \n",
       "2  Thanks. I’m listening to the audiobooks and to...           Related   \n",
       "3  Hey, I totally respect and appreciate this. Bu...           Related   \n",
       "4  Wasn't the End dimension only added to minecra...  Slightly Related   \n",
       "\n",
       "                                 generated_follow_up  \\\n",
       "0  [What are the primary sources of heat for anim...   \n",
       "1  [What are the potential long-term effects of f...   \n",
       "2  [What are some examples of the side conflicts ...   \n",
       "3  [What is the role of the human ear in interpre...   \n",
       "4  [Can you explain what \"sandbox game\" means in ...   \n",
       "\n",
       "                                     complete_answer  \\\n",
       "0  Yes, some animals can tan. Similar to humans, ...   \n",
       "1  GERD, or Gastroesophageal Reflux Disease, is a...   \n",
       "2  The dwarves did not actively participate in th...   \n",
       "3  When you speak into a phone, your voice genera...   \n",
       "4  Minecraft is an open-world sandbox game where ...   \n",
       "\n",
       "                   generated_follow_up_answerability  \n",
       "0  [[], [\"Complete Answer\"], [\"Complete Answer\"],...  \n",
       "1  [[“Original Answer”, “Complete Answer”], [], [...  \n",
       "2     [[“Original Answer”], [“Complete Answer”], []]  \n",
       "3  [[], [\"Original Answer\", \"Complete Answer\"], [...  \n",
       "4  [[\"Original Answer\", \"Complete Answer\"], [\"Com...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "follow_up_scores = {\n",
    "    \"full\": {\n",
    "        \"informative\":[],\n",
    "        \"not_informative\":[]\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"informative\":[],\n",
    "        \"not_informative\":[]\n",
    "    },\n",
    "    \"org\": {\n",
    "        \"informative\":[],\n",
    "        \"not_informative\":[]\n",
    "    },\n",
    "}\n",
    "\n",
    "informative_follow_up_scores = []\n",
    "not_informative_follow_up_scores = []\n",
    "ca = \"complete answer\"\n",
    "oa = \"original answer\"\n",
    "matches = 0\n",
    "\n",
    "for df_name, df in dfs_auto.items():\n",
    "    for _, data in df.iterrows():\n",
    "        id = data[\"id\"]\n",
    "        follow_up_idx = 1\n",
    "        follow_up_answerability = data[\"generated_follow_up_answerability\"].replace(\"“\", '\"').replace(\"”\", '\"').replace(\"'\", '\"')\n",
    "\n",
    "        if follow_up_answerability == ERROR_MSG: continue\n",
    "\n",
    "        follow_up_answerability = json.loads(follow_up_answerability)\n",
    "        \n",
    "        for follow_up in follow_up_answerability:\n",
    "            matching_row = df_new_info_human[\n",
    "                (df_new_info_human[\"prefix\"] == f\"{id}_{follow_up_idx}\") &\n",
    "                (df_new_info_human[\"group\"].str.startswith(df_name))\n",
    "            ]\n",
    "\n",
    "            if not matching_row.empty:\n",
    "                matches+=1\n",
    "                if follow_up and follow_up[0].lower() == ca:\n",
    "                    follow_up_scores[df_name][\"informative\"].extend(matching_row[\"newInformation\"].tolist())\n",
    "\n",
    "                    # informative_follow_up_scores.extend(matching_row[\"newInformation\"].tolist())\n",
    "                else:\n",
    "                    follow_up_scores[df_name][\"not_informative\"].extend(matching_row[\"newInformation\"].tolist())\n",
    "                    # not_informative_follow_up_scores.extend(matching_row[\"newInformation\"].tolist())\n",
    "                    \n",
    "            follow_up_idx+=1\n",
    "            \n",
    "print(matches)\n",
    "# len(informative_follow_up_scores + not_informative_follow_up_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_scores[\"org\"][\"not_informative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/Var of values obtained from Human Labelling for \n",
      "Questions classified as Informative/Not Informative by GPT-4o\n",
      "FULL\n",
      "                    Category      Mean  Variance\n",
      "0      Informative Follow-Up  1.421053  0.699908\n",
      "1  Not Informative Follow-Up  1.237500  0.881094\n",
      "GPT\n",
      "                    Category      Mean  Variance\n",
      "0      Informative Follow-Up  1.578947  0.998153\n",
      "1  Not Informative Follow-Up  1.398010  1.194822\n",
      "ORG\n",
      "                    Category      Mean  Variance\n",
      "0      Informative Follow-Up  0.861111  1.064043\n",
      "1  Not Informative Follow-Up  0.753894  0.926971\n"
     ]
    }
   ],
   "source": [
    "def informative_score_per_model(df_name):\n",
    "    data = {\n",
    "        \"Category\": [\"Informative Follow-Up\", \"Not Informative Follow-Up\"],\n",
    "        \"Mean\": [np.mean(follow_up_scores[df_name][\"informative\"]), np.mean(follow_up_scores[df_name][\"not_informative\"])],\n",
    "        \"Variance\": [np.var(follow_up_scores[df_name][\"informative\"]), np.var(follow_up_scores[df_name][\"not_informative\"])]\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_stats = pd.DataFrame(data)\n",
    "    df_stats\n",
    "    print(df_stats)\n",
    "    return df\n",
    "\n",
    "models = [\"full\", \"gpt\", \"org\"]\n",
    "\n",
    "print(\"Mean/Var of values obtained from Human Labelling for \\nQuestions classified as Informative/Not Informative by GPT-4o\")\n",
    "\n",
    "for model in models:\n",
    "    print(model.upper())\n",
    "    df_stats = informative_score_per_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/Var of values obtained from Human Labelling for \n",
      "Questions classified as Informative/Not Informative by GPT-4o\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Informative Follow-Up</td>\n",
       "      <td>1.294643</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Informative Follow-Up</td>\n",
       "      <td>1.076115</td>\n",
       "      <td>1.062448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Category      Mean  Variance\n",
       "0      Informative Follow-Up  1.294643  1.011400\n",
       "1  Not Informative Follow-Up  1.076115  1.062448"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean/Var of values obtained from Human Labelling for \\nQuestions classified as Informative/Not Informative by GPT-4o\")\n",
    "data = {\n",
    "    \"Category\": [\"Informative Follow-Up\", \"Not Informative Follow-Up\"],\n",
    "    \"Mean\": [np.mean(informative_follow_up_scores), np.mean(not_informative_follow_up_scores)],\n",
    "    \"Variance\": [np.var(informative_follow_up_scores), np.var(not_informative_follow_up_scores)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_stats = pd.DataFrame(data)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: 3.2887\n",
      "P-Value: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Perform independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(informative_follow_up_scores, not_informative_follow_up_scores, equal_var=False)\n",
    "\n",
    "# Print results\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's d: 0.2146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_diff = np.mean(informative_follow_up_scores) - np.mean(not_informative_follow_up_scores)\n",
    "pooled_std = np.sqrt((np.var(informative_follow_up_scores) + np.var(not_informative_follow_up_scores)) / 2)\n",
    "cohen_d = mean_diff / pooled_std\n",
    "\n",
    "print(f\"Cohen's d: {cohen_d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
