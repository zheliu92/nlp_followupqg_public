{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import krippendorff\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Excel\n",
    "filepath_release_1 = \"../Data/Mode1/Output/mode_1_release_1_all_output.xlsx\"\n",
    "filepath_release_2 = \"../Data/Mode1/Output/mode_1_release_2_all_output.xlsx\"\n",
    "filepath_release_3 = \"../Data/Mode1/Output/mode_1_release_3_all_output.xlsx\"\n",
    "raw_data_1 = pd.read_excel(filepath_release_1)\n",
    "raw_data_2 = pd.read_excel(filepath_release_2)\n",
    "raw_data_3 = pd.read_excel(filepath_release_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(raw_data):\n",
    "    processed_rows = []\n",
    "    \n",
    "    for _, row in raw_data.iterrows():\n",
    "        group = row[\"group\"]\n",
    "        task_data = json.loads(row[\"data\"])[\"Data\"][\"taskData\"][0]  # Parse the JSON data\n",
    "        \n",
    "        grouped_data = {}\n",
    "        for key, value in task_data.items():\n",
    "            prefix, topic = key.split('_', 1)\n",
    "            if prefix not in grouped_data:\n",
    "                grouped_data[prefix] = {}\n",
    "            grouped_data[prefix][topic] = value\n",
    "\n",
    "        # Convert grouped data into records for DataFrame\n",
    "        for prefix, topics in grouped_data.items():\n",
    "            record = {\"group\": group, \"prefix\": prefix, **topics}\n",
    "            processed_rows.append(record)\n",
    "    \n",
    "    return pd.DataFrame(processed_rows)\n",
    "\n",
    "def extract_true_values(df):\n",
    "    def find_true_value(mapping):\n",
    "        # Identify the key with a True value\n",
    "        for key, value in mapping.items():\n",
    "            if value:\n",
    "                return key\n",
    "        return None\n",
    "\n",
    "    # Apply to all topic columns\n",
    "    topic_columns = df.columns.difference([\"group\", \"prefix\"])\n",
    "    for column in topic_columns:\n",
    "        df[column] = df[column].apply(find_true_value)\n",
    "\n",
    "    return df\n",
    "\n",
    "def count_unique_values(df):\n",
    "    unique_counts = {}\n",
    "    for column in df.columns.difference([\"group\", \"prefix\"]):\n",
    "        unique_counts[column] = df[column].value_counts().to_dict()\n",
    "    return unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      group prefix validness       relatedness appropriateness\n",
      "0   group_1   3243       Yes  Slightly Related              No\n",
      "1   group_1   5230       Yes           Related              No\n",
      "2   group_1   8141       Yes           Related              No\n",
      "3   group_1  10373       Yes  Slightly Related              No\n",
      "4   group_1  10555       Yes           Related              No\n",
      "5   group_1  12564       Yes  Slightly Related              No\n",
      "6   group_1  12623       Yes           Related              No\n",
      "7   group_1  13094       Yes           Related              No\n",
      "8   group_1  13180       Yes           Related              No\n",
      "9   group_1  14406       Yes  Strongly Related              No\n",
      "10  group_1  15261       Yes  Slightly Related              No\n",
      "11  group_1  15478       Yes  Strongly Related              No\n",
      "12  group_1  17964       Yes  Slightly Related              No\n",
      "13  group_1  18333       Yes       Not Related              No\n",
      "14  group_1  18608       Yes       Not Related              No\n",
      "15  group_1  18763       Yes           Related              No\n",
      "16  group_1  19654       Yes       Not Related              No\n",
      "17  group_1  20157       Yes  Strongly Related              No\n",
      "18  group_1  20421       Yes           Related              No\n",
      "19  group_1  20709       Yes  Slightly Related              No\n",
      "20  group_2   1277       Yes       Not Related              No\n",
      "21  group_2   2969       Yes           Related             Yes\n",
      "22  group_2   3803       Yes           Related             Yes\n",
      "23  group_2   4027       Yes  Slightly Related              No\n",
      "24  group_2   4130       Yes       Not Related             Yes\n",
      "25  group_2   5462       Yes  Strongly Related             Yes\n",
      "26  group_2   7879       Yes       Not Related             Yes\n",
      "27  group_2   8581       Yes  Strongly Related             Yes\n",
      "28  group_2   9254       Yes  Slightly Related             Yes\n",
      "29  group_2  10865       Yes       Not Related              No\n",
      "30  group_2  12030       Yes  Slightly Related              No\n",
      "31  group_2  12934       Yes           Related             Yes\n",
      "32  group_2  13361       Yes  Slightly Related              No\n",
      "33  group_2  15499       Yes           Related             Yes\n",
      "34  group_2  15654       Yes  Strongly Related             Yes\n",
      "35  group_2  19350       Yes           Related             Yes\n",
      "36  group_2  20198       Yes  Slightly Related             Yes\n",
      "37  group_2  20802       Yes       Not Related             Yes\n",
      "38  group_2  24446       Yes  Slightly Related              No\n",
      "39  group_2  25047       Yes           Related              No\n",
      "40  group_3   1039       Yes  Slightly Related             Yes\n",
      "41  group_3   6082       Yes           Related              No\n",
      "42  group_3   7153       Yes  Strongly Related              No\n",
      "43  group_3   7313       Yes  Slightly Related              No\n",
      "44  group_3   8927       Yes           Related              No\n",
      "45  group_3   9000       Yes           Related              No\n",
      "46  group_3  10466       Yes  Slightly Related              No\n",
      "47  group_3  11686       Yes  Slightly Related              No\n",
      "48  group_3  12127       Yes           Related              No\n",
      "49  group_3  12737       Yes  Strongly Related              No\n",
      "50  group_3  14943       Yes  Slightly Related              No\n",
      "51  group_3  15446       Yes  Strongly Related              No\n",
      "52  group_3  15613       Yes           Related              No\n",
      "53  group_3  16021       Yes           Related              No\n",
      "54  group_3  17377       Yes       Not Related              No\n",
      "55  group_3  18317       Yes           Related              No\n",
      "56  group_3  18613       Yes  Strongly Related              No\n",
      "57  group_3  18811       Yes           Related              No\n",
      "58  group_3  22221       Yes           Related              No\n",
      "59  group_3  23660       Yes           Related              No\n",
      "60  group_4     28       Yes  Slightly Related              No\n",
      "61  group_4    296       Yes  Strongly Related              No\n",
      "62  group_4   3193       Yes  Strongly Related              No\n",
      "63  group_4   4543       Yes  Slightly Related              No\n",
      "64  group_4   4574       Yes           Related              No\n",
      "65  group_4   5153       Yes           Related              No\n",
      "66  group_4   5536       Yes           Related              No\n",
      "67  group_4   6502       Yes  Slightly Related              No\n",
      "68  group_4   7824       Yes           Related              No\n",
      "69  group_4   8836       Yes           Related              No\n",
      "70  group_4   9121       Yes  Slightly Related              No\n",
      "71  group_4  10118       Yes           Related              No\n",
      "72  group_4  10429       Yes           Related              No\n",
      "73  group_4  11314       Yes  Strongly Related              No\n",
      "74  group_4  13078       Yes  Strongly Related              No\n",
      "75  group_4  19365       Yes  Strongly Related              No\n",
      "76  group_4  19848       Yes           Related              No\n",
      "77  group_4  19912       Yes  Slightly Related              No\n",
      "78  group_4  19975       Yes  Strongly Related              No\n",
      "79  group_4  21245       Yes  Strongly Related              No\n",
      "80  group_5   1460       Yes  Strongly Related              No\n",
      "81  group_5   2092       Yes           Related              No\n",
      "82  group_5   2905       Yes  Strongly Related              No\n",
      "83  group_5   3293       Yes  Strongly Related              No\n",
      "84  group_5   3697       Yes  Strongly Related              No\n",
      "85  group_5   4712       Yes  Strongly Related              No\n",
      "86  group_5   4754       Yes  Strongly Related              No\n",
      "87  group_5   8081       Yes  Strongly Related              No\n",
      "88  group_5  10660       Yes           Related              No\n",
      "89  group_5  11793       Yes  Strongly Related             Yes\n",
      "90  group_5  12843       Yes  Strongly Related              No\n",
      "91  group_5  13658       Yes  Strongly Related              No\n",
      "92  group_5  16507       Yes  Strongly Related              No\n",
      "93  group_5  17684       Yes           Related              No\n",
      "94  group_5  18074       Yes  Strongly Related              No\n",
      "95  group_5  19758       Yes  Strongly Related              No\n",
      "96  group_5  23129       Yes  Strongly Related              No\n",
      "97  group_5  24508       Yes  Strongly Related             Yes\n",
      "98  group_5  24618       Yes  Strongly Related              No\n",
      "99  group_5  25372       Yes  Strongly Related              No\n",
      "\n",
      "Unique counts: {'appropriateness': {'No': 84, 'Yes': 16}, 'relatedness': {'Related': 35, 'Strongly Related': 34, 'Slightly Related': 22, 'Not Related': 9}, 'validness': {'Yes': 100}}\n"
     ]
    }
   ],
   "source": [
    "# Process and transform data\n",
    "processed_data_1 = process_raw_data(raw_data_1)\n",
    "processed_data_2 = process_raw_data(raw_data_2)\n",
    "processed_data_3 = process_raw_data(raw_data_3)\n",
    "\n",
    "# Extract only the \"True\" values\n",
    "data_with_true_values_1 = extract_true_values(processed_data_1)\n",
    "data_with_true_values_2 = extract_true_values(processed_data_2)\n",
    "data_with_true_values_3 = extract_true_values(processed_data_3)\n",
    "\n",
    "# Count unique values\n",
    "unique_counts_1 = count_unique_values(data_with_true_values_1)\n",
    "unique_counts_2 = count_unique_values(data_with_true_values_2)\n",
    "unique_counts_3 = count_unique_values(data_with_true_values_3)\n",
    "\n",
    "# Display the results\n",
    "print(data_with_true_values_1)\n",
    "print(\"\\nUnique counts:\", unique_counts_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(df):\n",
    "    # Define mappings\n",
    "    validness_map = {\"Yes\": 1, \"No\": 0}\n",
    "    relatedness_map = {\n",
    "        \"Strongly Related\": 3,\n",
    "        \"Related\": 2,\n",
    "        \"Slightly Related\": 1,\n",
    "        \"Not Related\": 0\n",
    "    }\n",
    "    appropriateness_map = {\"No\": 1, \"Yes\": 0}\n",
    "\n",
    "    # Apply mappings\n",
    "    df[\"validness\"] = df[\"validness\"].map(validness_map)\n",
    "    df[\"relatedness\"] = df[\"relatedness\"].map(relatedness_map)\n",
    "    df[\"appropriateness\"] = df[\"appropriateness\"].map(appropriateness_map)\n",
    "\n",
    "    return df.copy()\n",
    "\n",
    "df_1 = map_values(data_with_true_values_1)\n",
    "df_2 = map_values(data_with_true_values_2)\n",
    "df_3 = map_values(data_with_true_values_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release 1 Result Table: \n",
      "| group   |   prefix |   validness |   relatedness |   appropriateness |\n",
      "|:--------|---------:|------------:|--------------:|------------------:|\n",
      "| group_1 |     3243 |           1 |             1 |                 1 |\n",
      "| group_1 |     5230 |           1 |             2 |                 1 |\n",
      "| group_1 |     8141 |           1 |             2 |                 1 |\n",
      "| group_1 |    10373 |           1 |             1 |                 1 |\n",
      "| group_1 |    10555 |           1 |             2 |                 1 |\n",
      "| group_1 |    12564 |           1 |             1 |                 1 |\n",
      "| group_1 |    12623 |           1 |             2 |                 1 |\n",
      "| group_1 |    13094 |           1 |             2 |                 1 |\n",
      "| group_1 |    13180 |           1 |             2 |                 1 |\n",
      "| group_1 |    14406 |           1 |             3 |                 1 |\n",
      "\n",
      "Release 2 Result Table: \n",
      "| group   |   prefix |   validness |   relatedness |   appropriateness |\n",
      "|:--------|---------:|------------:|--------------:|------------------:|\n",
      "| group_1 |     3243 |           1 |             0 |                 1 |\n",
      "| group_1 |     5230 |           1 |             2 |                 1 |\n",
      "| group_1 |     8141 |           1 |             1 |                 1 |\n",
      "| group_1 |    10373 |           1 |             2 |                 1 |\n",
      "| group_1 |    10555 |           1 |             1 |                 1 |\n",
      "| group_1 |    12564 |           1 |             0 |                 1 |\n",
      "| group_1 |    12623 |           1 |             2 |                 1 |\n",
      "| group_1 |    13094 |           1 |             1 |                 1 |\n",
      "| group_1 |    13180 |           1 |             0 |                 1 |\n",
      "| group_1 |    14406 |           1 |             2 |                 1 |\n",
      "\n",
      "Release 3 Result Table: \n",
      "| group   |   prefix |   validness |   relatedness |   appropriateness |\n",
      "|:--------|---------:|------------:|--------------:|------------------:|\n",
      "| group_1 |     3243 |           1 |             3 |                 1 |\n",
      "| group_1 |     5230 |           1 |             2 |                 1 |\n",
      "| group_1 |     8141 |           1 |             2 |                 1 |\n",
      "| group_1 |    10373 |           1 |             1 |                 1 |\n",
      "| group_1 |    10555 |           1 |             3 |                 1 |\n",
      "| group_1 |    12564 |           1 |             2 |                 1 |\n",
      "| group_1 |    12623 |           1 |             3 |                 1 |\n",
      "| group_1 |    13094 |           1 |             2 |                 1 |\n",
      "| group_1 |    13180 |           1 |             3 |                 1 |\n",
      "| group_1 |    14406 |           1 |             3 |                 1 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Release 1 Result Table: \\n{df_1[:10].to_markdown(index=False)}\\n\")\n",
    "print(f\"Release 2 Result Table: \\n{df_2[:10].to_markdown(index=False)}\\n\")\n",
    "print(f\"Release 3 Result Table: \\n{df_3[:10].to_markdown(index=False)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      group prefix  validness  relatedness  appropriateness\n",
      "0   group_1   3243          1            1                1\n",
      "17  group_1  20157          1            3                1\n",
      "39  group_2  25047          1            2                1\n",
      "36  group_2  20198          1            1                0\n",
      "50  group_3  14943          1            1                1\n",
      "47  group_3  11686          1            1                1\n",
      "60  group_4     28          1            1                1\n",
      "78  group_4  19975          1            3                1\n",
      "85  group_5   4712          1            3                1\n",
      "82  group_5   2905          1            3                1\n",
      "group              object\n",
      "prefix             object\n",
      "validness           int64\n",
      "relatedness         int64\n",
      "appropriateness     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample test samples to do manual evaluation on\n",
    "data_with_true_values_test_samples = data_with_true_values_1.groupby('group').sample(n=2, random_state=42)\n",
    "\n",
    "data_with_true_values_test_samples.to_excel('test_samples_manual_evaluation.xlsx', index=False)\n",
    "\n",
    "print(data_with_true_values_test_samples)\n",
    "print(data_with_true_values_test_samples.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker Result: \n",
      "| group   |   prefix |   validness |   relatedness |   appropriateness |\n",
      "|:--------|---------:|------------:|--------------:|------------------:|\n",
      "| group_1 |     3243 |           1 |             1 |                 1 |\n",
      "| group_1 |    20157 |           1 |             3 |                 1 |\n",
      "| group_2 |    25047 |           1 |             2 |                 1 |\n",
      "| group_2 |    20198 |           1 |             1 |                 0 |\n",
      "| group_3 |    14943 |           1 |             1 |                 1 |\n",
      "| group_3 |    11686 |           1 |             1 |                 1 |\n",
      "| group_4 |       28 |           1 |             1 |                 1 |\n",
      "| group_4 |    19975 |           1 |             3 |                 1 |\n",
      "| group_5 |     4712 |           1 |             3 |                 1 |\n",
      "| group_5 |     2905 |           1 |             3 |                 1 |\n",
      "\n",
      "David Result: \n",
      "| group   |   prefix |   validness |   relatedness |   appropriateness |\n",
      "|:--------|---------:|------------:|--------------:|------------------:|\n",
      "| group_1 |     3243 |           1 |             1 |                 1 |\n",
      "| group_1 |    20157 |           1 |             2 |                 1 |\n",
      "| group_2 |    25047 |           1 |             1 |                 1 |\n",
      "| group_2 |    20198 |           1 |             2 |                 1 |\n",
      "| group_3 |    14943 |           1 |             2 |                 1 |\n",
      "| group_3 |    11686 |           1 |             1 |                 1 |\n",
      "| group_4 |       28 |           1 |             0 |                 1 |\n",
      "| group_4 |    19975 |           1 |             2 |                 1 |\n",
      "| group_5 |     4712 |           1 |             3 |                 1 |\n",
      "| group_5 |     2905 |           1 |             2 |                 1 |\n",
      "\n",
      "Zhe Result: \n",
      "| group   |   prefix |   validness |   relatedness |   appropriateness |\n",
      "|:--------|---------:|------------:|--------------:|------------------:|\n",
      "| group_1 |     3243 |           1 |             1 |                 1 |\n",
      "| group_1 |    20157 |           1 |             2 |                 1 |\n",
      "| group_2 |    25047 |           1 |             1 |                 1 |\n",
      "| group_2 |    20198 |           1 |             3 |                 1 |\n",
      "| group_3 |    14943 |           1 |             2 |                 1 |\n",
      "| group_3 |    11686 |           1 |             2 |                 1 |\n",
      "| group_4 |       28 |           1 |             1 |                 1 |\n",
      "| group_4 |    19975 |           1 |             1 |                 1 |\n",
      "| group_5 |     4712 |           1 |             3 |                 1 |\n",
      "| group_5 |     2905 |           1 |             2 |                 1 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Worker's Manual test sample evaluation\n",
    "worker_release_1_eval = pd.read_excel('/Users/tkang/Documents/research/nlp_followupqg/Human_Evaluation/Data/Mode1/Analysis/mode_1_release_1_manual_test_evaluation.xlsx', sheet_name=\"worker_release_1\")\n",
    "\n",
    "print(f\"Worker Result: \\n{worker_release_1_eval.to_markdown(index=False)}\\n\")\n",
    "\n",
    "# David's Manual test sample evaluation\n",
    "david_release_1_eval = pd.read_excel('/Users/tkang/Documents/research/nlp_followupqg/Human_Evaluation/Data/Mode1/Analysis/mode_1_release_1_manual_test_evaluation.xlsx', sheet_name=\"david_release_1\")\n",
    "\n",
    "print(f\"David Result: \\n{david_release_1_eval.to_markdown(index=False)}\\n\")\n",
    "\n",
    "# Zhe's Manual test sample evaluation\n",
    "zhe_release_1_eval = pd.read_excel('/Users/tkang/Documents/research/nlp_followupqg/Human_Evaluation/Data/Mode1/Analysis/mode_1_release_1_manual_test_evaluation.xlsx', sheet_name=\"zhe_release_1\")\n",
    "\n",
    "print(f\"Zhe Result: \\n{zhe_release_1_eval.to_markdown(index=False)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sample_questions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         {\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3243\u001b[39m,\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: Why electric cars don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use removable batteries?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms doable from a technical perspective, but it now means that you have to dramatically increase the total number of batteries in circulation as you need more than one for each vehicle on the road. Batteries are already a pretty extreme production and cost concern (roughly half the value of an EV is just the battery), and increasing the number of battery packs would inevitably mean increasing costs for the consumers who buy the EV.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat kind of infrastructure investments would be necessary for a widespread network of battery swapping stations?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         },\n\u001b[1;32m      9\u001b[0m         {\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20157\u001b[39m,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: Why does a debit increase an asset account?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou would not be left with $5k, you would be left with $15k.  You don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt debit \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m an asset account, you debit \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124minto\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m it.  A credit to an asset account is taking \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat other types of accounts, besides asset accounts, are affected by debits and credits?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m         }\n\u001b[1;32m     15\u001b[0m     },\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     17\u001b[0m         {\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m25047\u001b[39m,\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: what needs to happen before we can unmask safely?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are two answers. Neither of them very clear. One personally, and two society-wise. Both depend on risk. The first to your self , the second as a group.   Your risk is based on how likely you are to actually cross paths with someone infectious and how likely that is to cause you serious harm. So are you in an at risk group and where are you are considerations.   The second - how likely you are to be part of a chain of infection, how at risk are the people around you and how well society - such as the health service is coping.   Masks not perfect and life always holds a risk. In the West we have never masked up for flu which kills thousands of people a year - around 20,000 in the U.K. annually I think - though we vaccinate the at risk. Possibly COVId may have some nasty long term consequences even if you dint get seriously ill. But if the consequences of COVID dropped to equivalent to flu , then it would hardly be unreasonable to behave in a similar way. I say that nit to suggest we wait till then ( if it ever happened) but as a comparative starting point. No one can really say at what precise point in infections and consequences between that \u001b[39m\u001b[38;5;130;01m\\u2018\u001b[39;00m\u001b[38;5;124mflu\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124m point and the worst of COVID is the point at which we are \u001b[39m\u001b[38;5;130;01m\\u2018\u001b[39;00m\u001b[38;5;124msafe\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124m.   I expect government ps will come up with a balance of infection rates, hospitalisations and economics with a dose of politics.   Bear in mind that in the East people regularly wear masks on public transport so as either not to do catch viruses or not to spread them , I guess it\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124ms just considered normal hygiene even for colds.   That\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124ms a long winded way of saying that presuming you are not mandated by the governments and pretty much everyone has had access to a vaccine you will have to decide whether you feel either at risk yourself, or feel that you need to still protect other people.   Personally I feel like the vaccine plus some lowering of case rates make me feel safe enough - but I wear one sometimes out of consideration to people around me who are still wearing them or in particularly crowded and enclosed environments or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the benefits of using transparent face shields as an alternative to masks?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m         },\n\u001b[1;32m     23\u001b[0m         {\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20198\u001b[39m,\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meli5: What is confusion?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn many cases it is because of sensory overload. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms what happens when the brain gives up processing the input correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow do excessive and insufficient information contribute differently to confusion?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         },\n\u001b[1;32m     29\u001b[0m     },\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_3\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     31\u001b[0m         {\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m14943\u001b[39m,\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEli5 How do players go pro into esports?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you mean how do they develop the skills to be professional or what defines them as professional? The answer to the former is discipline and practice. The answer to the later is that they are sponsored and between sponsorships and winnings from tournaments make some or all of their living wage by gaming competitively.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are some key elements that mentorship might cover for aspiring professional esports players?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         },\n\u001b[1;32m     37\u001b[0m         {\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m11686\u001b[39m,\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: Why is it convention for websites to use boxes for questions with multiple answers, but dots for questions with single answers?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThey are two different types of controls.  The check boxes are independent, and you can check multiple boxes.  The dots are called radio buttons, and when they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre grouped together you can only check one.  Checking another will uncheck the previously checked one.  Since that functionality has already been built into the controls, it means less coding for the web designer.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does the use of checkboxes and radio buttons adhere to accessibility principles?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         },\n\u001b[1;32m     43\u001b[0m     },\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_4\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     45\u001b[0m         {\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m28\u001b[39m,\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: Why aren\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124mt there insects the size of man or larger?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThey don\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124mt breath the same way mammals do. They have tubes in various parts of their body that expose their circulatory system to air where it absorbs oxygen. Because they don\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124mt inhale, they air limited in size by the amount of oxygen in the atmosphere. Which is why they used to be bigger in past eras when there was more oxygen in the atmosphere.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does gravity affect the biomechanics of larger insects differently than smaller ones?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         },\n\u001b[1;32m     51\u001b[0m         {\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m19975\u001b[39m,\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: Why is deflation worse than inflation?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy would I pay $1000 for a TV today when it will be cheaper tomorrow.   Obviously not that simple.  But if people don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt spend money, shops go broke, people lose jobs etc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the mechanisms by which deflation can cause a credit crunch?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         },\n\u001b[1;32m     57\u001b[0m     },\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_5\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     59\u001b[0m         {\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4712\u001b[39m,\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meli5: Why do commonly used items, such as CRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms or old consoles skyrocket in value after they stop production? Assuming that these items stopped production today, why do sealed boxes or hell even good condition second-hand items almost double in price mere weeks after the end of production?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRTs have some advantages over LCDs and other flat-panel display technologies that matter for certain niche applications. To the typical consumer, the advantages of flat panels outweigh these rather specific disadvantages, and also different flat panel technologies have complementary strengths so most people don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt need a CRT. So once flat panels became cheap enough to be accessible to most consumers, CRT production took a nosedive. But the niche market for them remained, and so now you have a situation where the supply is almost 0 but there is still some demand for it. Of course, you could argue that supply could still track demand and so prices should be unaffected, but that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not the case because (1) with this big drop in production, economies of scale have been lost and so it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms more expensive to manufacture a CRT monitor these days and (2) the demand is no longer from average consumers who want to pay low prices for medium-quality products, but rather it comes from specialists or enthusiasts who need CRTs with specific, high-quality specifications and are willing to pay more for that.  Of course, the second point does not explain why even older, second-hand CRTs have gone up in price. That\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms more due to point (1) combined with a somewhat separate (though overlapping) demand from enthusiasts and collectors who don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt care so much about performance.  Anyway, in short: CRTs are an example of a product that has gotten more expensive because the mass market for it disappeared, and now it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms turned into a niche market, where costs are higher and customers are willing to pay more.  Things like old consoles, where production has fully and permanently stopped, are a different situation, because there you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre dealing with a somewhat steady (if small) demand combined with a steadily dwindling supply (that will eventually drop to 0). So e.g. if you really want to buy a NES today, your options are very limited as you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre dependent on the ever-decreasing number of NES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms in the world that still work, *and* that people are willing to sell. So even though very few people are looking to buy NES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms, the supply is even smaller.  I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm not sure it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms accurate to say that these consoles shoot up in price weeks after their production ends (that would surprise me but I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm happy to be corrected on that). By that point, presumably nearly everyone who wanted to buy one already did so, and people who couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt previously afford one now are looking to buy one cheaper in the second-hand market that is ramping up. Also, often consoles that go out of production are superseded by newer models and the bulk of the demand will switch to them, while the collectors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m market will take years to get off the ground. The only exception I can think of would be if the manufacturer (for some atypical reason) stopped producing the console before they had exhausted the (profitable) demand for it (and didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt release a new console yet).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does the release of newer console models impact the demand for older, discontinued ones?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         },\n\u001b[1;32m     65\u001b[0m         {\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2905\u001b[39m,\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELI5: why are there a huge amount of different insect varieties, like in ants, but only a small amount of different varieties in animals such as crocodiles?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTiny changes in those small animals allow them to fulfill different niches and avoid competition.  A tiny change in a crocodiles morphology wouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt do as much the separate it from the other croc species.    Ex.  A slight change in the size of a birds beak (I know I changed examples but still) will change its primary food source.  A small change in a crocs mouth.. will still make it be a top predator.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow-up\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does the reproductive rate of insects contribute to their high species diversity?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         },\n\u001b[1;32m     71\u001b[0m     }\n\u001b[1;32m     72\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "test_sample_questions = {\n",
    "    \"group_1\": {\n",
    "        {\n",
    "            \"id\": 3243,\n",
    "            \"question\": \"ELI5: Why electric cars don't use removable batteries?\",\n",
    "            \"answer\": \"It's doable from a technical perspective, but it now means that you have to dramatically increase the total number of batteries in circulation as you need more than one for each vehicle on the road. Batteries are already a pretty extreme production and cost concern (roughly half the value of an EV is just the battery), and increasing the number of battery packs would inevitably mean increasing costs for the consumers who buy the EV.\",\n",
    "            \"follow-up\": \"What kind of infrastructure investments would be necessary for a widespread network of battery swapping stations?\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 20157,\n",
    "            \"question\": \"ELI5: Why does a debit increase an asset account?\",\n",
    "            \"answer\": \"You would not be left with $5k, you would be left with $15k.  You don't debit \\\"from\\\" an asset account, you debit \\\"into\\\" it.  A credit to an asset account is taking \\\"from\\\" it.\",\n",
    "            \"follow-up\": \"What other types of accounts, besides asset accounts, are affected by debits and credits?\"\n",
    "        }\n",
    "    },\n",
    "    \"group_2\": {\n",
    "        {\n",
    "            \"id\": 25047,\n",
    "            \"question\": \"ELI5: what needs to happen before we can unmask safely?\",\n",
    "            \"answer\": \"There are two answers. Neither of them very clear. One personally, and two society-wise. Both depend on risk. The first to your self , the second as a group.   Your risk is based on how likely you are to actually cross paths with someone infectious and how likely that is to cause you serious harm. So are you in an at risk group and where are you are considerations.   The second - how likely you are to be part of a chain of infection, how at risk are the people around you and how well society - such as the health service is coping.   Masks not perfect and life always holds a risk. In the West we have never masked up for flu which kills thousands of people a year - around 20,000 in the U.K. annually I think - though we vaccinate the at risk. Possibly COVId may have some nasty long term consequences even if you dint get seriously ill. But if the consequences of COVID dropped to equivalent to flu , then it would hardly be unreasonable to behave in a similar way. I say that nit to suggest we wait till then ( if it ever happened) but as a comparative starting point. No one can really say at what precise point in infections and consequences between that \\u2018flu\\u2019 point and the worst of COVID is the point at which we are \\u2018safe\\u2019.   I expect government ps will come up with a balance of infection rates, hospitalisations and economics with a dose of politics.   Bear in mind that in the East people regularly wear masks on public transport so as either not to do catch viruses or not to spread them , I guess it\\u2019s just considered normal hygiene even for colds.   That\\u2019s a long winded way of saying that presuming you are not mandated by the governments and pretty much everyone has had access to a vaccine you will have to decide whether you feel either at risk yourself, or feel that you need to still protect other people.   Personally I feel like the vaccine plus some lowering of case rates make me feel safe enough - but I wear one sometimes out of consideration to people around me who are still wearing them or in particularly crowded and enclosed environments or both.\",\n",
    "            \"follow-up\": \"What are the benefits of using transparent face shields as an alternative to masks?\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 20198,\n",
    "            \"question\": \"eli5: What is confusion?\",\n",
    "            \"answer\": \"In many cases it is because of sensory overload. It's what happens when the brain gives up processing the input correctly.\",\n",
    "            \"follow-up\": \"How do excessive and insufficient information contribute differently to confusion?\"\n",
    "        },\n",
    "    },\n",
    "    \"group_3\": {\n",
    "        {\n",
    "            \"id\": 14943,\n",
    "            \"question\": \"Eli5 How do players go pro into esports?\",\n",
    "            \"answer\": \"Do you mean how do they develop the skills to be professional or what defines them as professional? The answer to the former is discipline and practice. The answer to the later is that they are sponsored and between sponsorships and winnings from tournaments make some or all of their living wage by gaming competitively.\",\n",
    "            \"follow-up\": \"What are some key elements that mentorship might cover for aspiring professional esports players?\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 11686,\n",
    "            \"question\": \"ELI5: Why is it convention for websites to use boxes for questions with multiple answers, but dots for questions with single answers?\",\n",
    "            \"answer\": \"They are two different types of controls.  The check boxes are independent, and you can check multiple boxes.  The dots are called radio buttons, and when they're grouped together you can only check one.  Checking another will uncheck the previously checked one.  Since that functionality has already been built into the controls, it means less coding for the web designer.\",\n",
    "            \"follow-up\": \"How does the use of checkboxes and radio buttons adhere to accessibility principles?\"\n",
    "        },\n",
    "    },\n",
    "    \"group_4\": {\n",
    "        {\n",
    "            \"id\": 28,\n",
    "            \"question\": \"ELI5: Why aren\\u2019t there insects the size of man or larger?\",\n",
    "            \"answer\": \"They don\\u2019t breath the same way mammals do. They have tubes in various parts of their body that expose their circulatory system to air where it absorbs oxygen. Because they don\\u2019t inhale, they air limited in size by the amount of oxygen in the atmosphere. Which is why they used to be bigger in past eras when there was more oxygen in the atmosphere.\",\n",
    "            \"follow-up\": \"How does gravity affect the biomechanics of larger insects differently than smaller ones?\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 19975,\n",
    "            \"question\": \"ELI5: Why is deflation worse than inflation?\",\n",
    "            \"answer\": \"Why would I pay $1000 for a TV today when it will be cheaper tomorrow.   Obviously not that simple.  But if people don't spend money, shops go broke, people lose jobs etc\",\n",
    "            \"follow-up\": \"What are the mechanisms by which deflation can cause a credit crunch?\"\n",
    "        },\n",
    "    },\n",
    "    \"group_5\": {\n",
    "        {\n",
    "            \"id\": 4712,\n",
    "            \"question\": \"eli5: Why do commonly used items, such as CRT's or old consoles skyrocket in value after they stop production? Assuming that these items stopped production today, why do sealed boxes or hell even good condition second-hand items almost double in price mere weeks after the end of production?\",\n",
    "            \"answer\": \"CRTs have some advantages over LCDs and other flat-panel display technologies that matter for certain niche applications. To the typical consumer, the advantages of flat panels outweigh these rather specific disadvantages, and also different flat panel technologies have complementary strengths so most people don't need a CRT. So once flat panels became cheap enough to be accessible to most consumers, CRT production took a nosedive. But the niche market for them remained, and so now you have a situation where the supply is almost 0 but there is still some demand for it. Of course, you could argue that supply could still track demand and so prices should be unaffected, but that's not the case because (1) with this big drop in production, economies of scale have been lost and so it's more expensive to manufacture a CRT monitor these days and (2) the demand is no longer from average consumers who want to pay low prices for medium-quality products, but rather it comes from specialists or enthusiasts who need CRTs with specific, high-quality specifications and are willing to pay more for that.  Of course, the second point does not explain why even older, second-hand CRTs have gone up in price. That's more due to point (1) combined with a somewhat separate (though overlapping) demand from enthusiasts and collectors who don't care so much about performance.  Anyway, in short: CRTs are an example of a product that has gotten more expensive because the mass market for it disappeared, and now it's turned into a niche market, where costs are higher and customers are willing to pay more.  Things like old consoles, where production has fully and permanently stopped, are a different situation, because there you're dealing with a somewhat steady (if small) demand combined with a steadily dwindling supply (that will eventually drop to 0). So e.g. if you really want to buy a NES today, your options are very limited as you're dependent on the ever-decreasing number of NES's in the world that still work, *and* that people are willing to sell. So even though very few people are looking to buy NES's, the supply is even smaller.  I'm not sure it's accurate to say that these consoles shoot up in price weeks after their production ends (that would surprise me but I'm happy to be corrected on that). By that point, presumably nearly everyone who wanted to buy one already did so, and people who couldn't previously afford one now are looking to buy one cheaper in the second-hand market that is ramping up. Also, often consoles that go out of production are superseded by newer models and the bulk of the demand will switch to them, while the collectors' market will take years to get off the ground. The only exception I can think of would be if the manufacturer (for some atypical reason) stopped producing the console before they had exhausted the (profitable) demand for it (and didn't release a new console yet).\",\n",
    "            \"follow-up\": \"How does the release of newer console models impact the demand for older, discontinued ones?\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2905,\n",
    "            \"question\": \"ELI5: why are there a huge amount of different insect varieties, like in ants, but only a small amount of different varieties in animals such as crocodiles?\",\n",
    "            \"answer\": \"Tiny changes in those small animals allow them to fulfill different niches and avoid competition.  A tiny change in a crocodiles morphology wouldn't do as much the separate it from the other croc species.    Ex.  A slight change in the size of a birds beak (I know I changed examples but still) will change its primary food source.  A small change in a crocs mouth.. will still make it be a top predator.\",\n",
    "            \"follow-up\": \"How does the reproductive rate of insects contribute to their high species diversity?\"\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'worker_release_1_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_worker \u001b[38;5;241m=\u001b[39m \u001b[43mworker_release_1_eval\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m df_david \u001b[38;5;241m=\u001b[39m david_release_1_eval\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m df_zhe \u001b[38;5;241m=\u001b[39m zhe_release_1_eval\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'worker_release_1_eval' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_worker = worker_release_1_eval.copy()\n",
    "df_david = david_release_1_eval.copy()\n",
    "df_zhe = zhe_release_1_eval.copy()\n",
    "\n",
    "def change_col_name(df, name):\n",
    "    df.columns = [\n",
    "        col + f'_{name}' if col not in ['group', 'prefix'] else col \n",
    "        for col in df.columns\n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "change_col_name(df_david, 'david')\n",
    "change_col_name(df_zhe, 'zhe')\n",
    "change_col_name(df_worker, 'worker')\n",
    "\n",
    "print(df_david)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>prefix</th>\n",
       "      <th>validness_david</th>\n",
       "      <th>relatedness_david</th>\n",
       "      <th>appropriateness_david</th>\n",
       "      <th>validness_zhe</th>\n",
       "      <th>relatedness_zhe</th>\n",
       "      <th>appropriateness_zhe</th>\n",
       "      <th>validness_worker</th>\n",
       "      <th>relatedness_worker</th>\n",
       "      <th>appropriateness_worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group_1</td>\n",
       "      <td>3243</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group_1</td>\n",
       "      <td>20157</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>group_2</td>\n",
       "      <td>20198</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group_2</td>\n",
       "      <td>25047</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group_3</td>\n",
       "      <td>11686</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>group_3</td>\n",
       "      <td>14943</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>group_4</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>group_4</td>\n",
       "      <td>19975</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>group_5</td>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>group_5</td>\n",
       "      <td>4712</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  prefix  validness_david  relatedness_david  appropriateness_david  \\\n",
       "0  group_1    3243                1                  1                      1   \n",
       "1  group_1   20157                1                  2                      1   \n",
       "2  group_2   20198                1                  2                      1   \n",
       "3  group_2   25047                1                  1                      1   \n",
       "4  group_3   11686                1                  1                      1   \n",
       "5  group_3   14943                1                  2                      1   \n",
       "6  group_4      28                1                  0                      1   \n",
       "7  group_4   19975                1                  2                      1   \n",
       "8  group_5    2905                1                  2                      1   \n",
       "9  group_5    4712                1                  3                      1   \n",
       "\n",
       "   validness_zhe  relatedness_zhe  appropriateness_zhe  validness_worker  \\\n",
       "0              1                1                    1                 1   \n",
       "1              1                2                    1                 1   \n",
       "2              1                3                    1                 1   \n",
       "3              1                1                    1                 1   \n",
       "4              1                2                    1                 1   \n",
       "5              1                2                    1                 1   \n",
       "6              1                1                    1                 1   \n",
       "7              1                1                    1                 1   \n",
       "8              1                2                    1                 1   \n",
       "9              1                3                    1                 1   \n",
       "\n",
       "   relatedness_worker  appropriateness_worker  \n",
       "0                   1                       1  \n",
       "1                   3                       1  \n",
       "2                   1                       0  \n",
       "3                   2                       1  \n",
       "4                   1                       1  \n",
       "5                   1                       1  \n",
       "6                   1                       1  \n",
       "7                   3                       1  \n",
       "8                   3                       1  \n",
       "9                   3                       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = df_david.merge(df_zhe, on=['group', 'prefix'], how='outer').merge(df_worker, on=['group', 'prefix'], how='outer')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_agreement_metrics(df, columns_to_check=['validness', 'relatedness', 'appropriateness']):\n",
    "    columns_to_check = ['validness', 'relatedness', 'appropriateness']\n",
    "    exclude_columns = ['group', 'prefix']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        david_column = f\"{column}_david\"\n",
    "        zhe_column = f\"{column}_zhe\"\n",
    "        worker_column = f\"{column}_worker\"\n",
    "\n",
    "        # Calculate percent agreement\n",
    "        agreement_david_zhe = (df[david_column] == df[zhe_column]).mean() * 100\n",
    "        agreement_david_worker = (df[david_column] == df[worker_column]).mean() * 100\n",
    "        agreement_zhe_worker = (df[zhe_column] == df[worker_column]).mean() * 100\n",
    "\n",
    "        fleiss_k = 200\n",
    "        kripp_alpha = 200\n",
    "        cohen_k_dz = 200\n",
    "        cohen_k_dw = 200\n",
    "        cohen_k_zw = 200\n",
    "\n",
    "        if column == \"relatedness\":\n",
    "            # Convert to numpy array for kappa calculations\n",
    "            # numeric_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "            # df[numeric_columns] = df[numeric_columns].astype(int)\n",
    "            \n",
    "            ratings = df.filter(like=\"relatedness\").to_numpy()\n",
    "\n",
    "            # print(ratings)\n",
    "            # print(np.apply_along_axis(lambda x: np.bincount(x, minlength=2), axis=1, arr=ratings))\n",
    "                  \n",
    "            # Compute Fleiss' Kappa\n",
    "            fleiss_k = fleiss_kappa(\n",
    "                np.apply_along_axis(lambda x: np.bincount(x), axis=1, arr=ratings)\n",
    "            )\n",
    "\n",
    "            # Compute Krippendorff's Alpha safely\n",
    "            unique_values = np.unique(ratings)\n",
    "            if len(unique_values) > 1:\n",
    "                kripp_alpha = krippendorff.alpha(ratings.T)\n",
    "            else:\n",
    "                kripp_alpha = np.nan  # Not computable\n",
    "\n",
    "            # Compute pairwise Cohen's Kappa\n",
    "            cohen_k_dz = cohen_kappa_score(df[david_column], df[zhe_column])\n",
    "            cohen_k_dw = cohen_kappa_score(df[david_column], df[worker_column])\n",
    "            cohen_k_zw = cohen_kappa_score(df[zhe_column], df[worker_column])\n",
    "\n",
    "        # Store results\n",
    "        results[column] = {\n",
    "            'david_zhe_percent_agreement': agreement_david_zhe,\n",
    "            'david_worker_percent_agreement': agreement_david_worker,\n",
    "            'zhe_worker_percent_agreement': agreement_zhe_worker,\n",
    "            'fleiss_kappa': fleiss_k,\n",
    "            'krippendorff_alpha': kripp_alpha,\n",
    "            'cohen_kappa_david_zhe': cohen_k_dz,\n",
    "            'cohen_kappa_david_worker': cohen_k_dw,\n",
    "            'cohen_kappa_zhe_worker': cohen_k_zw\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode 1 release 1\n",
      "Grouping by rule 'B' : 0 if 0 or 1, and 1 if 2 or 3\n",
      "Agreement Metrics for 'validness':\n",
      "  david_zhe_percent_agreement: 100.0000\n",
      "  david_worker_percent_agreement: 100.0000\n",
      "  zhe_worker_percent_agreement: 100.0000\n",
      "  fleiss_kappa: 200.0000\n",
      "  krippendorff_alpha: 200.0000\n",
      "  cohen_kappa_david_zhe: 200.0000\n",
      "  cohen_kappa_david_worker: 200.0000\n",
      "  cohen_kappa_zhe_worker: 200.0000\n",
      "Agreement Metrics for 'relatedness':\n",
      "  david_zhe_percent_agreement: 90.0000\n",
      "  david_worker_percent_agreement: 90.0000\n",
      "  zhe_worker_percent_agreement: 100.0000\n",
      "  fleiss_kappa: -0.0345\n",
      "  krippendorff_alpha: 0.0000\n",
      "  cohen_kappa_david_zhe: 0.0000\n",
      "  cohen_kappa_david_worker: 0.0000\n",
      "  cohen_kappa_zhe_worker: nan\n",
      "Agreement Metrics for 'appropriateness':\n",
      "  david_zhe_percent_agreement: 100.0000\n",
      "  david_worker_percent_agreement: 90.0000\n",
      "  zhe_worker_percent_agreement: 90.0000\n",
      "  fleiss_kappa: 200.0000\n",
      "  krippendorff_alpha: 200.0000\n",
      "  cohen_kappa_david_zhe: 200.0000\n",
      "  cohen_kappa_david_worker: 200.0000\n",
      "  cohen_kappa_zhe_worker: 200.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:716: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "# evaluating by converting 1,2,3 values to 1 and keeping 0 as-is for 'relatedness' column\n",
    "df = pd.DataFrame(merged_df)\n",
    "\n",
    "def recode_relatedness(column):\n",
    "    return column.apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Recode relatedness columns based on the disagreement rule\n",
    "df[\"relatedness_david\"] = recode_relatedness(df[\"relatedness_david\"])\n",
    "df[\"relatedness_zhe\"] = recode_relatedness(df[\"relatedness_zhe\"])\n",
    "df[\"relatedness_worker\"] = recode_relatedness(df[\"relatedness_worker\"])\n",
    "\n",
    "# Calculate agreement metrics\n",
    "agreement_metrics = calculate_agreement_metrics(df, columns_to_check)\n",
    "\n",
    "print(\"Mode 1 release 1\\nGrouping by rule 'B' : 0 if 0 or 1, and 1 if 2 or 3\")\n",
    "\n",
    "# Display results\n",
    "for column, metrics in agreement_metrics.items():\n",
    "    print(f\"Agreement Metrics for '{column}':\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode 1 release 1\n",
      "Grouping by rule 'B' : 0 if 0 or 1, and 1 if 2 or 3\n",
      "Agreement Metrics for 'validness':\n",
      "  david_zhe_percent_agreement: 100.0000\n",
      "  david_worker_percent_agreement: 100.0000\n",
      "  zhe_worker_percent_agreement: 100.0000\n",
      "  fleiss_kappa: 200.0000\n",
      "  krippendorff_alpha: 200.0000\n",
      "  cohen_kappa_david_zhe: 200.0000\n",
      "  cohen_kappa_david_worker: 200.0000\n",
      "  cohen_kappa_zhe_worker: 200.0000\n",
      "Agreement Metrics for 'relatedness':\n",
      "  david_zhe_percent_agreement: 100.0000\n",
      "  david_worker_percent_agreement: 100.0000\n",
      "  zhe_worker_percent_agreement: 100.0000\n",
      "  fleiss_kappa: nan\n",
      "  krippendorff_alpha: nan\n",
      "  cohen_kappa_david_zhe: nan\n",
      "  cohen_kappa_david_worker: nan\n",
      "  cohen_kappa_zhe_worker: nan\n",
      "Agreement Metrics for 'appropriateness':\n",
      "  david_zhe_percent_agreement: 100.0000\n",
      "  david_worker_percent_agreement: 90.0000\n",
      "  zhe_worker_percent_agreement: 90.0000\n",
      "  fleiss_kappa: 200.0000\n",
      "  krippendorff_alpha: 200.0000\n",
      "  cohen_kappa_david_zhe: 200.0000\n",
      "  cohen_kappa_david_worker: 200.0000\n",
      "  cohen_kappa_zhe_worker: 200.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/stats/inter_rater.py:266: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  kappa = (p_mean - p_mean_exp) / (1- p_mean_exp)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:716: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:716: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:716: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import krippendorff\n",
    "\n",
    "def recode_relatedness(column):\n",
    "    return column.apply(lambda x: 0 if (x == 0 or x == 1) else 1)\n",
    "\n",
    "# Recode relatedness columns based on the disagreement rule\n",
    "df[\"relatedness_david\"] = recode_relatedness(df[\"relatedness_david\"])\n",
    "df[\"relatedness_zhe\"] = recode_relatedness(df[\"relatedness_zhe\"])\n",
    "df[\"relatedness_worker\"] = recode_relatedness(df[\"relatedness_worker\"])\n",
    "\n",
    "# Calculate agreement metrics\n",
    "agreement_metrics = calculate_agreement_metrics(df, columns_to_check)\n",
    "\n",
    "print(\"Mode 1 release 1\\nGrouping by rule 'B' : 0 if 0 or 1, and 1 if 2 or 3\")\n",
    "\n",
    "# Display results\n",
    "for column, metrics in agreement_metrics.items():\n",
    "    print(f\"Agreement Metrics for '{column}':\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Metric  Mean  Variance\n",
      "0        validness  1.00  0.000000\n",
      "1      relatedness  1.94  0.925657\n",
      "2  appropriateness  0.84  0.135758\n",
      "            Metric  Mean  Variance\n",
      "0        validness  0.99  0.010000\n",
      "1      relatedness  2.05  1.159091\n",
      "2  appropriateness  0.94  0.056970\n",
      "            Metric  Mean  Variance\n",
      "0        validness  0.83  0.142525\n",
      "1      relatedness  2.22  0.819798\n",
      "2  appropriateness  0.98  0.019798\n",
      "Mode 1 Release 1 Mean and Variance:\n",
      "+-----------------+------------------+----------------------+------------------+----------------------+------------------+----------------------+\n",
      "| Metric          |   Mean_release_1 |   Variance_release_1 |   Mean_release_2 |   Variance_release_2 |   Mean_release_3 |   Variance_release_3 |\n",
      "+=================+==================+======================+==================+======================+==================+======================+\n",
      "| appropriateness |             0.84 |             0.135758 |             0.94 |            0.0569697 |             0.98 |             0.019798 |\n",
      "+-----------------+------------------+----------------------+------------------+----------------------+------------------+----------------------+\n",
      "| relatedness     |             1.94 |             0.925657 |             2.05 |            1.15909   |             2.22 |             0.819798 |\n",
      "+-----------------+------------------+----------------------+------------------+----------------------+------------------+----------------------+\n",
      "| validness       |             1    |             0        |             0.99 |            0.01      |             0.83 |             0.142525 |\n",
      "+-----------------+------------------+----------------------+------------------+----------------------+------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Mode 1 Mean and Variance\n",
    "def calculate_mean_var(df):\n",
    "    # Calculate mean and variance\n",
    "    mean_values = df[[\"validness\", \"relatedness\", \"appropriateness\"]].mean()\n",
    "    var_values = df[[\"validness\", \"relatedness\", \"appropriateness\"]].var()\n",
    "\n",
    "    # Create a DataFrame for results\n",
    "    result = pd.DataFrame(\n",
    "        [mean_values, var_values],\n",
    "        index=[\"Mean\", \"Variance\"]\n",
    "    )\n",
    "\n",
    "    # Transpose the result for better readability\n",
    "    result = result.T\n",
    "    result.reset_index(inplace=True)\n",
    "    result.rename(columns={\"index\": \"Metric\"}, inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "df_1_mean_var = calculate_mean_var(df_1)\n",
    "df_2_mean_var = calculate_mean_var(df_2)\n",
    "df_3_mean_var = calculate_mean_var(df_3)\n",
    "\n",
    "print(df_1_mean_var)\n",
    "print(df_2_mean_var)\n",
    "print(df_3_mean_var)\n",
    "\n",
    "combined_mean_var_table = pd.merge(df_1_mean_var, df_2_mean_var, on='Metric', how=\"outer\", suffixes=('_release_1', '_release_2'))\n",
    "combined_mean_var_table = pd.merge(combined_mean_var_table, df_3_mean_var, on='Metric', how=\"outer\")\n",
    "columns_to_rename = combined_mean_var_table.columns[-2:]\n",
    "rename_mapping = {col: f\"{col}_release_3\" for col in columns_to_rename}\n",
    "combined_mean_var_table = combined_mean_var_table.rename(columns=rename_mapping)\n",
    "\n",
    "# Pretty-print the table\n",
    "table = tabulate(\n",
    "    combined_mean_var_table,\n",
    "    headers=\"keys\",\n",
    "    tablefmt=\"grid\",\n",
    "    showindex=False\n",
    ")\n",
    "\n",
    "print(f\"Mode 1 Release 1 Mean and Variance:\\n{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean_release_1</th>\n",
       "      <th>Variance_release_1</th>\n",
       "      <th>Mean_release_2</th>\n",
       "      <th>Variance_release_2</th>\n",
       "      <th>Mean_release_3</th>\n",
       "      <th>Variance_release_3</th>\n",
       "      <th>Mean_across_releases</th>\n",
       "      <th>Variance_across_releases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appropriateness</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.135758</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.056970</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.070842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relatedness</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.925657</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.159091</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.819798</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.968182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>validness</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.142525</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.050842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metric  Mean_release_1  Variance_release_1  Mean_release_2  \\\n",
       "0  appropriateness            0.84            0.135758            0.94   \n",
       "1      relatedness            1.94            0.925657            2.05   \n",
       "2        validness            1.00            0.000000            0.99   \n",
       "\n",
       "   Variance_release_2  Mean_release_3  Variance_release_3  \\\n",
       "0            0.056970            0.98            0.019798   \n",
       "1            1.159091            2.22            0.819798   \n",
       "2            0.010000            0.83            0.142525   \n",
       "\n",
       "   Mean_across_releases  Variance_across_releases  \n",
       "0                  0.92                  0.070842  \n",
       "1                  2.07                  0.968182  \n",
       "2                  0.94                  0.050842  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Averagimg Across the Calculated Mean and Variance\n",
    "mean_columns = [col for col in combined_mean_var_table.columns if col.startswith(\"Mean_\")]\n",
    "var_columns = [col for col in combined_mean_var_table.columns if col.startswith(\"Variance_\")]\n",
    "combined_mean_var_table[\"Mean_across_releases\"] = combined_mean_var_table[mean_columns].mean(axis=1)\n",
    "combined_mean_var_table[\"Variance_across_releases\"] = combined_mean_var_table[var_columns].mean(axis=1)\n",
    "\n",
    "combined_mean_var_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode 1 ALL DATA COMBINED Mean and Variance:\n",
      "+-----------------+--------+------------+\n",
      "| Metric          |   Mean |   Variance |\n",
      "+=================+========+============+\n",
      "| validness       |   0.94 |  0.0565886 |\n",
      "+-----------------+--------+------------+\n",
      "| relatedness     |   2.07 |  0.975017  |\n",
      "+-----------------+--------+------------+\n",
      "| appropriateness |   0.92 |  0.0738462 |\n",
      "+-----------------+--------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Averagimg Across the Original Dataset\n",
    "# Combine df1 df2 and df3 into 1 table\n",
    "\n",
    "combined_df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
    "\n",
    "combined_df_mean_var = calculate_mean_var(combined_df)\n",
    "\n",
    "# Pretty-print the table\n",
    "table = tabulate(\n",
    "    combined_df_mean_var,\n",
    "    headers=\"keys\",\n",
    "    tablefmt=\"grid\",\n",
    "    showindex=False\n",
    ")\n",
    "\n",
    "print(f\"Mode 1 ALL DATA COMBINED Mean and Variance:\\n{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness\n",
      "3    129\n",
      "2     91\n",
      "1     52\n",
      "0     28\n",
      "Name: count, dtype: int64\n",
      "300\n",
      "0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "print(combined_df[\"relatedness\"].value_counts())\n",
    "\n",
    "print(len(combined_df))\n",
    "\n",
    "print((129+91+52)/300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['group', 'prefix', 'validness_release_1', 'relatedness_release_1',\n",
      "       'appropriateness_release_1', 'validness_release_2',\n",
      "       'relatedness_release_2', 'appropriateness_release_2',\n",
      "       'validness_release_3', 'relatedness_release_3',\n",
      "       'appropriateness_release_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Calculate Interannotator Agreement\n",
    "# converting column names and merging tables\n",
    "df_1_int = df_1.copy()\n",
    "df_2_int = df_2.copy()\n",
    "df_3_int = df_3.copy()\n",
    "\n",
    "def change_col_name(df, name):\n",
    "    df.columns = [\n",
    "        col + f'_{name}' if col not in ['group', 'prefix'] else col \n",
    "        for col in df.columns\n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "change_col_name(df_1_int, 'release_1')\n",
    "change_col_name(df_2_int, 'release_2')\n",
    "change_col_name(df_3_int, 'release_3')\n",
    "\n",
    "df_123_merged = df_1_int.merge(df_2_int, on=['group', 'prefix'], how='outer').merge(df_3_int, on=['group', 'prefix'], how='outer')\n",
    "\n",
    "print(df_123_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>prefix</th>\n",
       "      <th>validness_release_1</th>\n",
       "      <th>relatedness_release_1</th>\n",
       "      <th>appropriateness_release_1</th>\n",
       "      <th>validness_release_2</th>\n",
       "      <th>relatedness_release_2</th>\n",
       "      <th>appropriateness_release_2</th>\n",
       "      <th>validness_release_3</th>\n",
       "      <th>relatedness_release_3</th>\n",
       "      <th>appropriateness_release_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group_1</td>\n",
       "      <td>10373</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group_1</td>\n",
       "      <td>10555</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>group_1</td>\n",
       "      <td>12564</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group_1</td>\n",
       "      <td>12623</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group_1</td>\n",
       "      <td>13094</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>group_5</td>\n",
       "      <td>3293</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>group_5</td>\n",
       "      <td>3697</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>group_5</td>\n",
       "      <td>4712</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>group_5</td>\n",
       "      <td>4754</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>group_5</td>\n",
       "      <td>8081</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      group prefix  validness_release_1  relatedness_release_1  \\\n",
       "0   group_1  10373                    1                      1   \n",
       "1   group_1  10555                    1                      2   \n",
       "2   group_1  12564                    1                      1   \n",
       "3   group_1  12623                    1                      2   \n",
       "4   group_1  13094                    1                      2   \n",
       "..      ...    ...                  ...                    ...   \n",
       "95  group_5   3293                    1                      3   \n",
       "96  group_5   3697                    1                      3   \n",
       "97  group_5   4712                    1                      3   \n",
       "98  group_5   4754                    1                      3   \n",
       "99  group_5   8081                    1                      3   \n",
       "\n",
       "    appropriateness_release_1  validness_release_2  relatedness_release_2  \\\n",
       "0                           1                    1                      2   \n",
       "1                           1                    1                      1   \n",
       "2                           1                    1                      0   \n",
       "3                           1                    1                      2   \n",
       "4                           1                    1                      1   \n",
       "..                        ...                  ...                    ...   \n",
       "95                          1                    1                      3   \n",
       "96                          1                    1                      3   \n",
       "97                          1                    1                      3   \n",
       "98                          1                    1                      3   \n",
       "99                          1                    1                      2   \n",
       "\n",
       "    appropriateness_release_2  validness_release_3  relatedness_release_3  \\\n",
       "0                           1                    1                      1   \n",
       "1                           1                    1                      3   \n",
       "2                           1                    1                      2   \n",
       "3                           1                    1                      3   \n",
       "4                           1                    1                      2   \n",
       "..                        ...                  ...                    ...   \n",
       "95                          1                    1                      3   \n",
       "96                          1                    0                      3   \n",
       "97                          1                    0                      3   \n",
       "98                          1                    0                      3   \n",
       "99                          1                    1                      3   \n",
       "\n",
       "    appropriateness_release_3  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           1  \n",
       "3                           1  \n",
       "4                           1  \n",
       "..                        ...  \n",
       "95                          1  \n",
       "96                          1  \n",
       "97                          1  \n",
       "98                          1  \n",
       "99                          1  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_123_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_agreement_metrics_between_annotators(df, columns_to_check):\n",
    "    results = {}\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        release_1 = f\"{column}_release_1\"\n",
    "        release_2 = f\"{column}_release_2\"\n",
    "        release_3 = f\"{column}_release_3\"\n",
    "\n",
    "        # print(type(df[release_1]))\n",
    "\n",
    "        # Compute pairwise Cohen's Kappa\n",
    "        cohen_k_1_2 = cohen_kappa_score(df[release_1], df[release_2])\n",
    "        cohen_k_1_3 = cohen_kappa_score(df[release_1], df[release_2])\n",
    "        cohen_k_2_3 = cohen_kappa_score(df[release_3], df[release_3])\n",
    "        \n",
    "        # Calculate pairwise percent agreement\n",
    "        agreement_1_2 = (df[release_1] == df[release_2]).mean() * 100\n",
    "        agreement_1_3 = (df[release_1] == df[release_3]).mean() * 100\n",
    "        agreement_2_3 = (df[release_2] == df[release_3]).mean() * 100\n",
    "\n",
    "        agreement_all_3 = df[[release_1, release_2, release_3]].apply(lambda row: row.nunique() == 1, axis=1).mean() * 100\n",
    "\n",
    "        # fleiss_k = 200\n",
    "        # kripp_alpha = 200\n",
    "        # cohen_k_dz = 200\n",
    "        # cohen_k_dw = 200\n",
    "        # cohen_k_zw = 200\n",
    "            \n",
    "        ratings = df.filter(like=column).to_numpy()\n",
    "                \n",
    "        # # Compute Fleiss' Kappa\n",
    "        fleiss_k = fleiss_kappa(\n",
    "            np.apply_along_axis(lambda x: np.bincount(x, minlength=2), axis=1, arr=ratings)\n",
    "        )\n",
    "\n",
    "        # Compute Krippendorff's Alpha safely\n",
    "        unique_values = np.unique(ratings)\n",
    "        if len(unique_values) > 1:\n",
    "            kripp_alpha = krippendorff.alpha(ratings.T)\n",
    "            kripp_alpha_1_2 = krippendorff.alpha(ratings[:,0:2].T)\n",
    "            kripp_alpha_1_3 = krippendorff.alpha(ratings[:, 1].T)\n",
    "            kripp_alpha_2_3 = krippendorff.alpha(ratings[:,1:3].T)\n",
    "        else:\n",
    "            kripp_alpha = np.nan  # Not computable\n",
    "\n",
    "        pabak_1_2 = 2 * ((df[release_1] == df[release_2]).mean()) - 1\n",
    "        pabak_1_3 = 2 * ((df[release_1] == df[release_3]).mean()) - 1\n",
    "        pabak_2_3 = 2 * ((df[release_2] == df[release_3]).mean()) - 1\n",
    "\n",
    "        # Store results\n",
    "        results[column] = {\n",
    "            '1_2_percent_agreement': agreement_1_2,\n",
    "            '1_3_percent_agreement': agreement_1_3,\n",
    "            '2_3_percent_agreement': agreement_2_3,\n",
    "            'percent_agreement_all_3': agreement_all_3,\n",
    "            'fleiss_kappa': fleiss_k,\n",
    "            'krippendorff_alpha': kripp_alpha,\n",
    "            'cohen_kappa_1_2': cohen_k_1_2,\n",
    "            'cohen_kappa_1_3': cohen_k_1_3,\n",
    "            'cohen_kappa_2_3': cohen_k_2_3,\n",
    "            'kripp_alpha_1_2': kripp_alpha_1_2,\n",
    "            'kripp_alpha_1_3': kripp_alpha_1_3,\n",
    "            'kripp_alpha_2_3': kripp_alpha_2_3,\n",
    "            'pabak_1_2': pabak_1_2,\n",
    "            'pabak_1_3': pabak_1_3,\n",
    "            'pabak_2_3': pabak_2_3,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Per Row: {'validness': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0.21875]], 'relatedness': [[0.02777777777777768, 0.45945945945945954, 0.21875], [0.2727272727272727, -0.16666666666666674, -0.0714285714285714], [1.0, 1.0, 1.0], [1, 1, 0.5], [1, 1, 1]], 'appropriateness': [[1, 1, 1], [-0.04838709677419373, 1, 1], [1.0, -0.05263157894736836, -0.05263157894736836], [1, 1, 1], [0.7727272727272727, -0.0714285714285714, -0.08108108108108092]]}\n",
      "Krippendorff's Alpha per Row: {'validness': [1, 1, -0.1132075471698113, -0.0535714285714286, 0.04582210242587603], 'relatedness': [0.17400000000000004, 0.007211538461538436, 1.0, 0.20485175202156336, 1], 'appropriateness': [1, -0.22370370370370374, 0.3099415204678362, 1, 0.2716049382716049]}\n",
      "Cohen's Kappa Average per Row: {'validness': [1.0, 1.0, 1.0, 1.0, 0.7395833333333334], 'relatedness': [0.23532907907907907, 0.011544011544011523, 1.0, 0.8333333333333334, 1.0], 'appropriateness': [1.0, 0.6505376344086021, 0.29824561403508776, 1.0, 0.2067392067392068]}\n",
      "Krippendorff's Alpha Average per Row: {'validness': [1.0, 1.0, -0.1132075471698113, -0.0535714285714286, 0.04582210242587603], 'relatedness': [0.17400000000000004, 0.007211538461538436, 1.0, 0.20485175202156336, 1.0], 'appropriateness': [1.0, -0.22370370370370374, 0.3099415204678362, 1.0, 0.2716049382716049]}\n",
      "Average of all Cohen's Kappa for column validness: 0.9479166666666666\n",
      "Average of all Cohen's Kappa for column relatedness: 0.6160412847912848\n",
      "Average of all Cohen's Kappa for column appropriateness: 0.6311044910365793\n",
      "Average of all Krippendorff's Alpha for column validness: 0.3758086253369272\n",
      "Average of all Krippendorff's Alpha for column relatedness: 0.47721265809662033\n",
      "Average of all Krippendorff's Alpha for column appropriateness: 0.47156855100714745\n",
      "Average of all questions cohen's Kappa: 0.7316874808315103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_34557/3759787067.py:119: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[relatedness_cols] = df[relatedness_cols].applymap(lambda x: int(x != 0))\n",
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_34557/3759787067.py:119: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[relatedness_cols] = df[relatedness_cols].applymap(lambda x: int(x != 0))\n",
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_34557/3759787067.py:119: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[relatedness_cols] = df[relatedness_cols].applymap(lambda x: int(x != 0))\n",
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_34557/3759787067.py:119: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[relatedness_cols] = df[relatedness_cols].applymap(lambda x: int(x != 0))\n",
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_34557/3759787067.py:119: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[relatedness_cols] = df[relatedness_cols].applymap(lambda x: int(x != 0))\n"
     ]
    }
   ],
   "source": [
    "### Jan 31 - Redo kappa calculation\n",
    "def calculate_agreement_scores(df, kappas_per_row, alpha_per_row):\n",
    "    results = {}\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        release_1 = f\"{column}_release_1\"\n",
    "        release_2 = f\"{column}_release_2\"\n",
    "        release_3 = f\"{column}_release_3\"\n",
    "\n",
    "        # Compute pairwise Cohen's Kappa\n",
    "        # cohen_k_1_2 = cohen_kappa_score(df[release_1], df[release_2])\n",
    "        # cohen_k_1_3 = cohen_kappa_score(df[release_1], df[release_2])\n",
    "        # cohen_k_2_3 = cohen_kappa_score(df[release_3], df[release_3])\n",
    "        \n",
    "        # Calculate pairwise percent agreement\n",
    "        agreement_1_2 = (df[release_1] == df[release_2]).mean() * 100\n",
    "        agreement_1_3 = (df[release_1] == df[release_3]).mean() * 100\n",
    "        agreement_2_3 = (df[release_2] == df[release_3]).mean() * 100\n",
    "\n",
    "        agreement_all_3 = df[[release_1, release_2, release_3]].apply(lambda row: row.nunique() == 1, axis=1).mean() * 100\n",
    "            \n",
    "        ratings = df.filter(like=column).to_numpy()\n",
    "        \n",
    "        kripp_alpha = 1\n",
    "        # kripp_alpha_1_2 = np.nan\n",
    "        # kripp_alpha_1_3 = np.nan\n",
    "        # kripp_alpha_2_3 = np.nan\n",
    "        \n",
    "        # Compute Krippendorff's Alpha safely\n",
    "        unique_values = np.unique(ratings)\n",
    "        if len(unique_values) > 1:\n",
    "            kripp_alpha = krippendorff.alpha(ratings.T)\n",
    "            # kripp_alpha_1_2 = krippendorff.alpha(ratings[:,0:2].T)\n",
    "            # kripp_alpha_1_3 = krippendorff.alpha(ratings[:, 1].T)\n",
    "            # kripp_alpha_2_3 = krippendorff.alpha(ratings[:,1:3].T)\n",
    "        else:\n",
    "            kripp_alpha = 1  # Not computable\n",
    "            # kripp_alpha_1_2 = 1\n",
    "            # kripp_alpha_1_3 = 1\n",
    "            # kripp_alpha_2_3 = 1\n",
    "\n",
    "        cohen_k_1_2 = 1\n",
    "        cohen_k_1_3 = 1\n",
    "        cohen_k_2_3 = 1\n",
    "\n",
    "        if len(set(df[release_1])) > 1 and len(set(df[release_2])) > 1:\n",
    "            cohen_k_1_2 = cohen_kappa_score(df[release_1], df[release_2])\n",
    "        if len(set(df[release_1])) > 1 and len(set(df[release_3])) > 1:\n",
    "            cohen_k_1_3 = cohen_kappa_score(df[release_1], df[release_3])\n",
    "        if len(set(df[release_2])) > 1 and len(set(df[release_3])) > 1:\n",
    "            cohen_k_2_3 = cohen_kappa_score(df[release_2], df[release_3])\n",
    "        \n",
    "\n",
    "        pabak_1_2 = 2 * ((df[release_1] == df[release_2]).mean()) - 1\n",
    "        pabak_1_3 = 2 * ((df[release_1] == df[release_3]).mean()) - 1\n",
    "        pabak_2_3 = 2 * ((df[release_2] == df[release_3]).mean()) - 1\n",
    "\n",
    "        kappas_per_row[column].append([cohen_k_1_2, cohen_k_1_3, cohen_k_2_3])\n",
    "        alpha_per_row[column].append(kripp_alpha)\n",
    "\n",
    "        # Store results\n",
    "        results[column] = {\n",
    "            '1_2_percent_agreement': agreement_1_2,\n",
    "            '1_3_percent_agreement': agreement_1_3,\n",
    "            '2_3_percent_agreement': agreement_2_3,\n",
    "            'percent_agreement_all_3': agreement_all_3,\n",
    "            'krippendorff_alpha': kripp_alpha,\n",
    "            'cohen_kappa_1_2': cohen_k_1_2,\n",
    "            'cohen_kappa_1_3': cohen_k_1_3,\n",
    "            'cohen_kappa_2_3': cohen_k_2_3,\n",
    "            # 'kripp_alpha_1_2': kripp_alpha_1_2,\n",
    "            # 'kripp_alpha_1_3': kripp_alpha_1_3,\n",
    "            # 'kripp_alpha_2_3': kripp_alpha_2_3,\n",
    "            'pabak_1_2': pabak_1_2,\n",
    "            'pabak_1_3': pabak_1_3,\n",
    "            'pabak_2_3': pabak_2_3,\n",
    "        }\n",
    "\n",
    "    return results    \n",
    "\n",
    "# Calculate Interannotator Agreement\n",
    "# converting column names and merging tables\n",
    "df_1_int = df_1.copy()\n",
    "df_2_int = df_2.copy()\n",
    "df_3_int = df_3.copy()\n",
    "\n",
    "def change_col_name(df, name):\n",
    "    df.columns = [\n",
    "        col + f'_{name}' if col not in ['group', 'prefix'] else col \n",
    "        for col in df.columns\n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "change_col_name(df_1_int, 'release_1')\n",
    "change_col_name(df_2_int, 'release_2')\n",
    "change_col_name(df_3_int, 'release_3')\n",
    "\n",
    "rel_1_A = df_1[0:20]\n",
    "kappas_per_row = {\n",
    "    \"validness\": [],\n",
    "    \"relatedness\": [],\n",
    "    \"appropriateness\": [],\n",
    "}\n",
    "alpha_per_row = {\n",
    "    \"validness\": [],\n",
    "    \"relatedness\": [],\n",
    "    \"appropriateness\": [],\n",
    "}\n",
    "\n",
    "columns_to_check=['validness', 'relatedness', 'appropriateness']\n",
    "\n",
    "for i in range(5):\n",
    "    df = df_1_int[i*20:(i+1)*20].merge(df_2_int[i*20:(i+1)*20], on=['group', 'prefix'], how='outer').merge(df_3_int[i*20:(i+1)*20], on=['group', 'prefix'], how='outer')\n",
    "\n",
    "    relatedness_cols = [col for col in df.columns if col.startswith(\"relatedness_\")]\n",
    "\n",
    "    # Apply transformation to all matching columns (Binning rule A)\n",
    "    df[relatedness_cols] = df[relatedness_cols].applymap(lambda x: int(x != 0))\n",
    "\n",
    "    results = calculate_agreement_scores(df, kappas_per_row, alpha_per_row)\n",
    "\n",
    "    # print(\"Mode 1 release 1, 2, 3 \\nGrouping by rule 'A': 0 if 0, 1 otherwise\")\n",
    "    # # Display results\n",
    "    # for column, metrics in results.items():\n",
    "    #     print(f\"\\nMetrics for '{column}':\")\n",
    "    #     for key, value in metrics.items():\n",
    "    #         print(f\"  {key}: {value:.5f}\")\n",
    "\n",
    "    # df = None\n",
    "\n",
    "print(f\"Cohen's Kappa Per Row: {kappas_per_row}\")\n",
    "print(f\"Krippendorff's Alpha per Row: {alpha_per_row}\")\n",
    "\n",
    "average_of_all_kappas = {\n",
    "    \"validness\": [],\n",
    "    \"relatedness\": [],\n",
    "    \"appropriateness\": [],\n",
    "}\n",
    "average_of_all_alphas = {\n",
    "    \"validness\": [],\n",
    "    \"relatedness\": [],\n",
    "    \"appropriateness\": [],\n",
    "}\n",
    "\n",
    "# for each Cohen's Kappa set, take the average\n",
    "for column, values in kappas_per_row.items():\n",
    "    for value in values:\n",
    "        average_of_all_kappas[column].append(np.mean(value))\n",
    "\n",
    "for column, values in alpha_per_row.items():\n",
    "    for value in values:\n",
    "        average_of_all_alphas[column].append(np.mean(value))\n",
    "\n",
    "print(f\"Cohen's Kappa Average per Row: {average_of_all_kappas}\")\n",
    "print(f\"Krippendorff's Alpha Average per Row: {average_of_all_alphas}\")\n",
    "\n",
    "total_value = 0\n",
    "# for each Cohen's Kappa set, take the average\n",
    "for column, values in average_of_all_kappas.items():\n",
    "    print(f\"Average of all Cohen's Kappa for column {column}: {np.mean(values)}\")\n",
    "    total_value += np.mean(values)\n",
    "\n",
    "for column, values in average_of_all_alphas.items():\n",
    "    print(f\"Average of all Krippendorff's Alpha for column {column}: {np.mean(values)}\")\n",
    "\n",
    "print(f\"Average of all questions cohen's Kappa: {total_value / 3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode 1 release 1, 2, 3 \n",
      "Grouping by rule 'A': 0 if 0, 1 otherwise\n",
      "\n",
      "Metrics for 'validness':\n",
      "  1_2_percent_agreement: 99.00000\n",
      "  1_3_percent_agreement: 83.00000\n",
      "  2_3_percent_agreement: 84.00000\n",
      "  percent_agreement_all_3: 83.00000\n",
      "  fleiss_kappa: -0.00473\n",
      "  krippendorff_alpha: -0.00138\n",
      "  cohen_kappa_1_2: 0.00000\n",
      "  cohen_kappa_1_3: 0.00000\n",
      "  cohen_kappa_2_3: 1.00000\n",
      "  kripp_alpha_1_2: 0.00000\n",
      "  kripp_alpha_1_3: 0.00000\n",
      "  kripp_alpha_2_3: 0.02808\n",
      "  pabak_1_2: 0.98000\n",
      "  pabak_1_3: 0.66000\n",
      "  pabak_2_3: 0.68000\n",
      "\n",
      "Metrics for 'relatedness':\n",
      "  1_2_percent_agreement: 84.00000\n",
      "  1_3_percent_agreement: 89.00000\n",
      "  2_3_percent_agreement: 89.00000\n",
      "  percent_agreement_all_3: 81.00000\n",
      "  fleiss_kappa: 0.25158\n",
      "  krippendorff_alpha: 0.25407\n",
      "  cohen_kappa_1_2: 0.18616\n",
      "  cohen_kappa_1_3: 0.18616\n",
      "  cohen_kappa_2_3: 1.00000\n",
      "  kripp_alpha_1_2: 0.18693\n",
      "  kripp_alpha_1_3: 0.00000\n",
      "  kripp_alpha_2_3: 0.36348\n",
      "  pabak_1_2: 0.68000\n",
      "  pabak_1_3: 0.78000\n",
      "  pabak_2_3: 0.78000\n",
      "\n",
      "Metrics for 'appropriateness':\n",
      "  1_2_percent_agreement: 86.00000\n",
      "  1_3_percent_agreement: 82.00000\n",
      "  2_3_percent_agreement: 92.00000\n",
      "  percent_agreement_all_3: 80.00000\n",
      "  fleiss_kappa: 0.09420\n",
      "  krippendorff_alpha: 0.09722\n",
      "  cohen_kappa_1_2: 0.30279\n",
      "  cohen_kappa_1_3: 0.30279\n",
      "  cohen_kappa_2_3: 1.00000\n",
      "  kripp_alpha_1_2: 0.28856\n",
      "  kripp_alpha_1_3: 0.00000\n",
      "  kripp_alpha_2_3: -0.03646\n",
      "  pabak_1_2: 0.72000\n",
      "  pabak_1_3: 0.64000\n",
      "  pabak_2_3: 0.84000\n"
     ]
    }
   ],
   "source": [
    "# evaluating by converting 1,2,3 values to 1 and keep 0 as 0 for the 'relatedness' column\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "df = pd.DataFrame(df_123_merged.copy())\n",
    "\n",
    "# print(pd.DataFrame(df[\"validness_release_1\"], df[\"validness_release_2\"]))\n",
    "\n",
    "# kappa = cohen_kappa_score(df[\"validness_release_1\"], df[\"validness_release_2\"])\n",
    "# # print(kappa)\n",
    "\n",
    "# observed_agreement = (df[\"validness_release_1\"] == df[\"validness_release_2\"]).mean()\n",
    "# pabak = 2 * observed_agreement - 1\n",
    "# print(f\"PABAK Score: {pabak:.4f}\")\n",
    "\n",
    "# print(f\"Krippendorff’s Alpha: {kripp_alpha:.4f}\")\n",
    "\n",
    "# Function to recode 'relatedness' based on the disagreement rule\n",
    "def recode_relatedness(column):\n",
    "    return column.apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Recode 'relatedness' columns\n",
    "df[\"relatedness_release_1\"] = recode_relatedness(df[\"relatedness_release_1\"])\n",
    "df[\"relatedness_release_2\"] = recode_relatedness(df[\"relatedness_release_2\"])\n",
    "df[\"relatedness_release_3\"] = recode_relatedness(df[\"relatedness_release_3\"])\n",
    "\n",
    "columns_to_check = ['validness', 'relatedness', 'appropriateness']\n",
    "\n",
    "# Calculate percent agreement and Cohen's Kappa\n",
    "results = calculate_agreement_metrics_between_annotators(df, columns_to_check)\n",
    "\n",
    "print(\"Mode 1 release 1, 2, 3 \\nGrouping by rule 'A': 0 if 0, 1 otherwise\")\n",
    "# Display results\n",
    "for column, metrics in results.items():\n",
    "    print(f\"\\nMetrics for '{column}':\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value:.5f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode 1 release 1,2,3 \n",
      "Grouping by rule 'B' : 0 if 0 or 1, 1 otherwise\n",
      "Percent Agreement for 'validness':\n",
      "  1_2_percent_agreement: 99.00\n",
      "  1_3_percent_agreement: 83.00\n",
      "  2_3_percent_agreement: 84.00\n",
      "  percent_agreement_all_3: 83.00\n",
      "  fleiss_kappa: -0.00\n",
      "  krippendorff_alpha: -0.00\n",
      "  cohen_kappa_1_2: 0.00\n",
      "  cohen_kappa_1_3: 0.00\n",
      "  cohen_kappa_2_3: 1.00\n",
      "  kripp_alpha_1_2: 0.00\n",
      "  kripp_alpha_1_3: 0.00\n",
      "  kripp_alpha_2_3: 0.03\n",
      "Percent Agreement for 'relatedness':\n",
      "  1_2_percent_agreement: 70.00\n",
      "  1_3_percent_agreement: 71.00\n",
      "  2_3_percent_agreement: 73.00\n",
      "  percent_agreement_all_3: 57.00\n",
      "  fleiss_kappa: 0.27\n",
      "  krippendorff_alpha: 0.27\n",
      "  cohen_kappa_1_2: 0.29\n",
      "  cohen_kappa_1_3: 0.29\n",
      "  cohen_kappa_2_3: 1.00\n",
      "  kripp_alpha_1_2: 0.29\n",
      "  kripp_alpha_1_3: 0.00\n",
      "  kripp_alpha_2_3: 0.27\n",
      "Percent Agreement for 'appropriateness':\n",
      "  1_2_percent_agreement: 86.00\n",
      "  1_3_percent_agreement: 82.00\n",
      "  2_3_percent_agreement: 92.00\n",
      "  percent_agreement_all_3: 80.00\n",
      "  fleiss_kappa: 0.09\n",
      "  krippendorff_alpha: 0.10\n",
      "  cohen_kappa_1_2: 0.30\n",
      "  cohen_kappa_1_3: 0.30\n",
      "  cohen_kappa_2_3: 1.00\n",
      "  kripp_alpha_1_2: 0.29\n",
      "  kripp_alpha_1_3: 0.00\n",
      "  kripp_alpha_2_3: -0.04\n"
     ]
    }
   ],
   "source": [
    "# evaluating by converting 2,3 values to 1 and converting 0,1 values to 0 for the 'relatedness' column\n",
    "df = pd.DataFrame(df_123_merged.copy())\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "def recode_relatedness_2(column):\n",
    "    return column.apply(lambda x: 0 if x in [0,1] else 1)\n",
    "\n",
    "# Recode relatedness columns based on the disagreement rule\n",
    "df[\"relatedness_release_1\"] = recode_relatedness_2(df[\"relatedness_release_1\"])\n",
    "df[\"relatedness_release_2\"] = recode_relatedness_2(df[\"relatedness_release_2\"])\n",
    "df[\"relatedness_release_3\"] = recode_relatedness_2(df[\"relatedness_release_3\"])\n",
    "\n",
    "columns_to_check = ['validness', 'relatedness', 'appropriateness']\n",
    "\n",
    "# Calculate percent agreement\n",
    "percent_agreement = calculate_agreement_metrics_between_annotators(df, columns_to_check)\n",
    "\n",
    "print(\"Mode 1 release 1,2,3 \\nGrouping by rule 'B' : 0 if 0 or 1, 1 otherwise\")\n",
    "# Display results\n",
    "for column, agreements in percent_agreement.items():\n",
    "    print(f\"Percent Agreement for '{column}':\")\n",
    "    for key, value in agreements.items():\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
