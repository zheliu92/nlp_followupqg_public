{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data size of 909 reduced to -> 155\n"
     ]
    }
   ],
   "source": [
    "### Human Evaluator Verified Dataset\n",
    "data = pd.read_csv(\"output_mode_2_only_valid_questions.csv\")\n",
    "og_data_size = len(data)\n",
    "\n",
    "df_gp = data[[\"group\", \"prefix\"]]\n",
    "df_gp = df_gp.drop_duplicates(subset=[\"prefix\"])\n",
    "print(f\"Original Data size of {og_data_size} reduced to -> {len(df_gp)}\")\n",
    "\n",
    "# group by the first part of the prefix - e.g 3000\n",
    "df_gp['prefix'], df_gp['index'] = zip(*df_gp['prefix'].str.split('_').apply(lambda x: (x[0], x[1])))\n",
    "df_gp['index'] = df_gp['index'].astype(int) - 1\n",
    "df_gp['group'] = df_gp['group'].str.split('_').str[0]\n",
    "df_gp = df_gp.groupby(['group', 'prefix'])['index'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Human Evaluator Verified Dataset\n",
    "# algorithm to parse through each group and every prefix to collect a list of OQ/OA/CA/FUQS\n",
    "full_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/full_clustered.json'\n",
    "gpt_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/gpt_clustered.json'\n",
    "org_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/org_clustered.json'\n",
    "\n",
    "# Load data\n",
    "full_df = pd.read_json(full_path)\n",
    "gpt_df = pd.read_json(gpt_path)\n",
    "org_df = pd.read_json(org_path)\n",
    "\n",
    "filtered_data_full = pd.DataFrame(columns=full_df.columns)\n",
    "filtered_data_gpt = pd.DataFrame(columns=gpt_df.columns)\n",
    "filtered_data_org = pd.DataFrame(columns=org_df.columns)\n",
    "\n",
    "json_df = None\n",
    "\n",
    "# Explode the `generated_follow_up` column\n",
    "# json_data = json_data.explode('generated_follow_up', ignore_index=True)\n",
    "\n",
    "for index, row in df_gp.iterrows():\n",
    "    match row['group']:\n",
    "        case 'full':\n",
    "            json_df = full_df\n",
    "        case 'gpt':\n",
    "            print()\n",
    "            json_df = gpt_df\n",
    "        case 'org':\n",
    "            json_df = org_df\n",
    "        case _:\n",
    "            print(\"Invalid File Found\")\n",
    "            json_df = None\n",
    "            break\n",
    "    \n",
    "    for _, json_data in json_df.iterrows():\n",
    "        if int(json_data['id']) != int(row['prefix']):\n",
    "            continue\n",
    "        \n",
    "        relevant_follow_ups = np.array(json_data['generated_follow_up'])\n",
    "        relevant_follow_ups = relevant_follow_ups[row['index']]\n",
    "        row_data = json_data\n",
    "        row_data['generated_follow_up'] = relevant_follow_ups\n",
    "\n",
    "        match row['group']:\n",
    "            case 'full':\n",
    "                filtered_data_full.loc[len(filtered_data_full)] = row_data\n",
    "            case 'gpt':\n",
    "                filtered_data_gpt.loc[len(filtered_data_gpt)] = row_data\n",
    "            case 'org':\n",
    "                filtered_data_org.loc[len(filtered_data_org)] = row_data\n",
    "            case _:\n",
    "                print(\"Invalid File Found\")\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entire Dataset Collected by Each of the 3 Models\n",
    "full_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/full_clustered.json'\n",
    "gpt_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/gpt_clustered.json'\n",
    "org_path = '/Users/tkang/Documents/research/nlp_followupqg/Auto_Evaluation/org_clustered.json'\n",
    "\n",
    "# Load data\n",
    "full_df = pd.read_json(full_path)\n",
    "gpt_df = pd.read_json(gpt_path)\n",
    "org_df = pd.read_json(org_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tkang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tkang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Methods to Determine if a Sentence is a valid question\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a regex pattern to match informative questions\n",
    "invalid_words_pattern = r'<\\w+>'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def contains_question_mark(sentence):\n",
    "    return sentence[-1] == '?'\n",
    "\n",
    "def is_question_dependency_parsing(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"aux\" and token.head.dep_ == \"ROOT\":\n",
    "            return True  # Finds auxiliary verbs like \"is\", \"does\" in questions\n",
    "        if token.dep_ == \"attr\" and token.head.dep_ == \"ROOT\":\n",
    "            return True  # Finds WH-questions like \"What is...\"\n",
    "\n",
    "def contains_invalid_words(question):\n",
    "    return not bool(re.search(invalid_words_pattern, question))\n",
    "\n",
    "def get_word_sequences(sentence):\n",
    "    \"\"\"Returns a set of word sequences of at least `min_length` words from a sentence.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)  # Extract words\n",
    "    sequences = set()\n",
    "\n",
    "    for i in range(len(words) - min_length + 1):\n",
    "        phrase = \" \".join(words[i:i + min_length])  # Create word sequence\n",
    "        sequences.add(phrase)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def contains_duplicate_words(og_question, og_answer, follow_up_question):\n",
    "    \"\"\"Checks if there is a common substring of more than `min_length` words between two sentences.\"\"\"\n",
    "    og_question_answer = og_question + og_answer\n",
    "    \n",
    "    og_seq = get_word_sequences(og_question_answer)\n",
    "    follow_up_seq = get_word_sequences(follow_up_question)\n",
    "\n",
    "    return not bool(og_seq.intersection(follow_up_seq))  # Find common sequences\n",
    "\n",
    "# combining all the other methods\n",
    "def is_valid_question(question, og_question, og_answer):\n",
    "    # print(question)\n",
    "    return (\n",
    "        contains_question_mark(question) and \n",
    "        is_question_dependency_parsing(question) and \n",
    "        contains_invalid_words(question) and\n",
    "        contains_duplicate_words(og_question, og_answer, question)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_question = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_json(\"full_df.json\", orient=\"records\", indent=4)\n",
    "gpt_df.to_json(\"gpt_df.json\", orient=\"records\", indent=4)\n",
    "org_df.to_json(\"org_df.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering out all invalid follow up questions in FULL: 2061\n",
      "after filtering out all invalid follow up questions in FULL: 2011\n",
      "before filtering out all invalid follow up questions in GPT: 1895\n",
      "after filtering out all invalid follow up questions in GPT: 1877\n",
      "before filtering out all invalid follow up questions in ORG: 2349\n",
      "after filtering out all invalid follow up questions in ORG: 2067\n"
     ]
    }
   ],
   "source": [
    "def filterInvalidFollowUpQuestions(df):\n",
    "    # df columns = ['id', 'question', 'answer', 'follow-up', 'relation', 'generated_follow_up']\n",
    "    for index, row in df.iterrows():\n",
    "        valid_questions = [follow_up for follow_up in row['generated_follow_up'] if is_valid_question(follow_up)]\n",
    "        df.at[index, \"generated_follow_up\"] = valid_questions\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"before filtering out all invalid follow up questions in FULL: { len(full_df['generated_follow_up'].explode()) }\")\n",
    "full_df_valid_follow_up_only = filterInvalidFollowUpQuestions(full_df.copy())\n",
    "print(f\"after filtering out all invalid follow up questions in FULL: {len(full_df_valid_follow_up_only['generated_follow_up'].explode())}\")\n",
    "\n",
    "print(f\"before filtering out all invalid follow up questions in GPT: {len(gpt_df['generated_follow_up'].explode())}\")\n",
    "gpt_df_valid_follow_up_only = filterInvalidFollowUpQuestions(gpt_df.copy())\n",
    "print(f\"after filtering out all invalid follow up questions in GPT: {len(gpt_df_valid_follow_up_only['generated_follow_up'].explode())}\")\n",
    "\n",
    "print(f\"before filtering out all invalid follow up questions in ORG: {len(org_df['generated_follow_up'].explode())}\")\n",
    "full_org_valid_follow_up_only = filterInvalidFollowUpQuestions(org_df.copy())\n",
    "print(f\"after filtering out all invalid follow up questions in ORG: {len(full_org_valid_follow_up_only['generated_follow_up'].explode())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = full_df_valid_follow_up_only.to_json(orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to /Users/tkang/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.76049675 0.74630396 0.7439385  0.73491124 0.73727811]\n",
      "Mean Accuracy: 0.7445857113363823\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download dataset\n",
    "nltk.download('nps_chat')\n",
    "posts = nltk.corpus.nps_chat.xml_posts()\n",
    "\n",
    "# Extract text and labels\n",
    "posts_text = [post.text for post in posts]\n",
    "y = [post.get('class') for post in posts]\n",
    "\n",
    "# Split into train and test (80-20 split)\n",
    "train_text = posts_text[:int(len(posts_text) * 0.8)]\n",
    "test_text = posts_text[int(len(posts_text) * 0.2):]\n",
    "\n",
    "y_train = y[:int(len(posts_text) * 0.8)]\n",
    "y_test = y[int(len(posts_text) * 0.2):]\n",
    "\n",
    "# Get TF-IDF features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), \n",
    "                             min_df=0.001, \n",
    "                             max_df=0.7, \n",
    "                             analyzer='word')\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_text)\n",
    "X_test = vectorizer.transform(test_text)\n",
    "\n",
    "# Define classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=400, random_state=0)\n",
    "\n",
    "# Use 5-fold cross-validation on the training set\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(gb, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey there asl please?', 'i jus wanna know sumfin really important ....']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Accept       0.80      0.71      0.75       167\n",
      "         Bye       0.89      0.77      0.83       155\n",
      "     Clarify       0.62      0.33      0.43        24\n",
      "   Continuer       0.65      0.43      0.52       115\n",
      "     Emotion       0.94      0.67      0.78       868\n",
      "    Emphasis       0.82      0.48      0.61       132\n",
      "       Greet       0.97      0.91      0.94      1044\n",
      "       Other       0.00      0.00      0.00        32\n",
      "      Reject       0.88      0.70      0.78       122\n",
      "   Statement       0.73      0.94      0.82      2505\n",
      "      System       0.99      0.98      0.99      2279\n",
      "     nAnswer       0.69      0.76      0.72        58\n",
      "  whQuestion       0.90      0.88      0.89       432\n",
      "     yAnswer       0.79      0.64      0.71        89\n",
      "  ynQuestion       0.93      0.62      0.74       432\n",
      "\n",
      "    accuracy                           0.86      8454\n",
      "   macro avg       0.77      0.65      0.70      8454\n",
      "weighted avg       0.87      0.86      0.86      8454\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "gb.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = gb.predict(X_test)\n",
    "\n",
    "#Accuracy of 86% not bad\n",
    "print(classification_report(y_test, predictions_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
